{"title":"Decima-Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers","slug":"Robustness-Analysis-and-Enhancement-of-Deep-Reinforcement-Learning-Based-Schedulers","date":"2023-01-15T17:03:38.000Z","updated":"2023-01-16T07:11:23.399Z","comments":true,"path":"api/articles/Robustness-Analysis-and-Enhancement-of-Deep-Reinforcement-Learning-Based-Schedulers.json","excerpt":"Decima(Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers)解决的问题我们设计了黑盒扰动系统，其中训练了一个代理模型来模仿基于 DRL 的调度策略，并且表明，高可信代理模型可以帮助制作有效的扰动。扰动的意思是对作业的节点特性或依赖性进行轻微调整，同时不改变其功能。最终，我们研究了提高基于 DRL 的调度程序对此类扰动的鲁棒性的解决方案：我们提出了一种对抗性训练框架，以强制神经模型在训练过程中适应扰动模式，从而消除应用过程中的潜在损害。","covers":null,"content":"<h2 id=\"Decima-Robustness-Analysis-and-Enhancement-of-Deep-Reinforcement-Learning-Based-Schedulers\"><a href=\"#Decima-Robustness-Analysis-and-Enhancement-of-Deep-Reinforcement-Learning-Based-Schedulers\" class=\"headerlink\" title=\"Decima(Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers)\"></a>Decima(Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers)</h2><h3 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h3><p>我们设计了黑盒扰动系统，其中训练了一个代理模型来模仿基于 DRL 的调度策略，并且表明，高可信代理模型可以帮助制作有效的扰动。扰动的意思是对作业的节点特性或依赖性进行轻微调整，同时不改变其功能。</p>\n<p>最终，我们研究了提高基于 DRL 的调度程序对此类扰动的鲁棒性的解决方案：我们提出了一种对抗性训练框架，以强制神经模型在训练过程中适应扰动模式，从而消除应用过程中的潜在损害。</p>\n<span id=\"more\"></span>\n\n<ul>\n<li><strong>提出问题</strong></li>\n</ul>\n<p>各种研究发现深度神经模型（如DRL模型）容易受到对抗性数据实例的影响（如其观察空间或动作空间输入的扰动）产生错误决策，并且缺乏鲁棒性；而在云计算调度问题中，对鲁棒性也有很高的要求，即使系统中没有恶意用户也有可能会有一些特征模式触发调度程序的不当行为，因此这种鲁棒性问题并不总是与对抗性扰动相关联，但研究它的重要性是绝对的。</p>\n<ul>\n<li><strong>解决思路</strong></li>\n</ul>\n<ol>\n<li>研究如何开发一种有效扰乱工作特征的方法。</li>\n<li>在成功模拟有效扰乱工作特征之后，提出对抗训练方法以提升模型的鲁棒性</li>\n</ol>\n<ul>\n<li><strong>实验结果</strong></li>\n</ul>\n<p>我们的实验表明，这种鲁棒性的提高显着降低了工作扰动的成功率。即使扰动成功，它也会降低扰动作业的性能增益。具有对抗性训练的 DRL 调度器能够实现与原始 DRL 调度器相当的调度性能。</p>\n<h3 id=\"黑盒扰动系统\"><a href=\"#黑盒扰动系统\" class=\"headerlink\" title=\"黑盒扰动系统\"></a>黑盒扰动系统</h3>","more":"<ul>\n<li><strong>提出问题</strong></li>\n</ul>\n<p>各种研究发现深度神经模型（如DRL模型）容易受到对抗性数据实例的影响（如其观察空间或动作空间输入的扰动）产生错误决策，并且缺乏鲁棒性；而在云计算调度问题中，对鲁棒性也有很高的要求，即使系统中没有恶意用户也有可能会有一些特征模式触发调度程序的不当行为，因此这种鲁棒性问题并不总是与对抗性扰动相关联，但研究它的重要性是绝对的。</p>\n<ul>\n<li><strong>解决思路</strong></li>\n</ul>\n<ol>\n<li>研究如何开发一种有效扰乱工作特征的方法。</li>\n<li>在成功模拟有效扰乱工作特征之后，提出对抗训练方法以提升模型的鲁棒性</li>\n</ol>\n<ul>\n<li><strong>实验结果</strong></li>\n</ul>\n<p>我们的实验表明，这种鲁棒性的提高显着降低了工作扰动的成功率。即使扰动成功，它也会降低扰动作业的性能增益。具有对抗性训练的 DRL 调度器能够实现与原始 DRL 调度器相当的调度性能。</p>\n<h3 id=\"黑盒扰动系统\"><a href=\"#黑盒扰动系统\" class=\"headerlink\" title=\"黑盒扰动系统\"></a>黑盒扰动系统</h3>","categories":[],"tags":[{"name":"python","path":"api/tags/python.json"},{"name":"强化学习","path":"api/tags/强化学习.json"}]}