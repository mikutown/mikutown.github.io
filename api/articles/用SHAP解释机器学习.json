{"title":"用SHAP解释机器学习","slug":"用SHAP解释机器学习","date":"2022-08-09T09:41:28.000Z","updated":"2022-08-09T10:18:53.833Z","comments":true,"path":"api/articles/用SHAP解释机器学习.json","excerpt":null,"covers":null,"content":"<h1 id=\"用SHAP解释机器学习\"><a href=\"#用SHAP解释机器学习\" class=\"headerlink\" title=\"用SHAP解释机器学习\"></a>用SHAP解释机器学习</h1><h2 id=\"什么是Explainable-AI？\"><a href=\"#什么是Explainable-AI？\" class=\"headerlink\" title=\"什么是Explainable AI？\"></a>什么是Explainable AI？</h2><p>关于要如何解释自己模型预测出来的结果，我们都需要了解为何我们训练出的模型会做出如此判断，是基于什么原因作出这种判断，会不会model完全用了非常诡异的特征去下决策（但说不定是对的？）以下归纳出几个我们为什么想知道我们训练出来的模型到底在说什么？</p>\n<ul>\n<li>确认模型的合理性：在我们需要做出决策，倚重模型做判断时，我们需要知道，这个模型所参考的数据特征是否正如我们想象的一样。若我们不知道演算法建议的理由，那下决策肯定会觉得害怕，如同依赖了一个名为AI的黑盒子一般。</li>\n<li>改良模型：这点其实和第一点略为相同，如果我们能知道我们的模型所预测的依据，我们就可以去试着改善它。特别是当模型和资料中，存在了一些恰恰好的bias，若我们没试着看背后的原因的话，通常很难发现一些问题。以过去的专案经验当作例子，我们使用影像检验在找瑕疵物件时，预测的效果不错，但取细看背后的原因，却发现模型依赖背景而做出决断。详细的情况是物件的缺陷在拍摄当天时有个固定的光影，模型判断defect的依据竟然是用那个光影，而非物件上的瑕疵。</li>\n<li>从模型上学习：当模型真正的从品质良好的训练资料集中得到了有用的判断依据，人类就可以从模型中学到一些东西。例如为期，人类目前很显然已经被AI击败，但在无关输赢的时候，职业棋手目前早已寻求AI的帮助，让自己的技术更上一层楼。</li>\n</ul>\n<h2 id=\"SHAP-Value\"><a href=\"#SHAP-Value\" class=\"headerlink\" title=\"SHAP Value\"></a>SHAP Value</h2><p>以上几点让我们了解Explainable AI的必要性，那我们该如何下手去理解每个预测我们的模型是如何理解的呢？</p>\n<p>SHAP values（SHapley Additive exPlanations）是一个Python的视觉化分析套件，让我们能轻易地了解我们的模型做出决策的依据。</p>\n<p>那对我们来说，什么时候该用SHAP value呢？</p>\n<p>举例：</p>\n<ul>\n<li>制造业的入料控制以达到最佳化结果，你如何正确地减少不必要的入料，也可以达到一样的产能。</li>\n<li>利用Users的使用行为，找到诈骗账号。并对模型做出解释，借此在各种features中找到诈骗账号一般性具有的行为。</li>\n</ul>\n","more":"<h1 id=\"用SHAP解释机器学习\"><a href=\"#用SHAP解释机器学习\" class=\"headerlink\" title=\"用SHAP解释机器学习\"></a>用SHAP解释机器学习</h1><h2 id=\"什么是Explainable-AI？\"><a href=\"#什么是Explainable-AI？\" class=\"headerlink\" title=\"什么是Explainable AI？\"></a>什么是Explainable AI？</h2><p>关于要如何解释自己模型预测出来的结果，我们都需要了解为何我们训练出的模型会做出如此判断，是基于什么原因作出这种判断，会不会model完全用了非常诡异的特征去下决策（但说不定是对的？）以下归纳出几个我们为什么想知道我们训练出来的模型到底在说什么？</p>\n<ul>\n<li>确认模型的合理性：在我们需要做出决策，倚重模型做判断时，我们需要知道，这个模型所参考的数据特征是否正如我们想象的一样。若我们不知道演算法建议的理由，那下决策肯定会觉得害怕，如同依赖了一个名为AI的黑盒子一般。</li>\n<li>改良模型：这点其实和第一点略为相同，如果我们能知道我们的模型所预测的依据，我们就可以去试着改善它。特别是当模型和资料中，存在了一些恰恰好的bias，若我们没试着看背后的原因的话，通常很难发现一些问题。以过去的专案经验当作例子，我们使用影像检验在找瑕疵物件时，预测的效果不错，但取细看背后的原因，却发现模型依赖背景而做出决断。详细的情况是物件的缺陷在拍摄当天时有个固定的光影，模型判断defect的依据竟然是用那个光影，而非物件上的瑕疵。</li>\n<li>从模型上学习：当模型真正的从品质良好的训练资料集中得到了有用的判断依据，人类就可以从模型中学到一些东西。例如为期，人类目前很显然已经被AI击败，但在无关输赢的时候，职业棋手目前早已寻求AI的帮助，让自己的技术更上一层楼。</li>\n</ul>\n<h2 id=\"SHAP-Value\"><a href=\"#SHAP-Value\" class=\"headerlink\" title=\"SHAP Value\"></a>SHAP Value</h2><p>以上几点让我们了解Explainable AI的必要性，那我们该如何下手去理解每个预测我们的模型是如何理解的呢？</p>\n<p>SHAP values（SHapley Additive exPlanations）是一个Python的视觉化分析套件，让我们能轻易地了解我们的模型做出决策的依据。</p>\n<p>那对我们来说，什么时候该用SHAP value呢？</p>\n<p>举例：</p>\n<ul>\n<li>制造业的入料控制以达到最佳化结果，你如何正确地减少不必要的入料，也可以达到一样的产能。</li>\n<li>利用Users的使用行为，找到诈骗账号。并对模型做出解释，借此在各种features中找到诈骗账号一般性具有的行为。</li>\n</ul>\n","categories":[],"tags":[{"name":"python","path":"api/tags/python.json"},{"name":"机器学习基础","path":"api/tags/机器学习基础.json"},{"name":"博弈论基础","path":"api/tags/博弈论基础.json"}]}