{"title":"Decima-Learning Scheduling Algorithms for Data Processing Clusters","slug":"Decima-Learning-Scheduling-Algorithms-for-Data-Processing-Clusters","date":"2023-01-09T09:39:18.000Z","updated":"2023-01-16T10:58:19.595Z","comments":true,"path":"api/articles/Decima-Learning-Scheduling-Algorithms-for-Data-Processing-Clusters.json","excerpt":"Decima(Learning Scheduling Algorithms for Data Processing Clusters):用强化学习解决调度问题解决的问题解决了在云上的多个以DAG图表示的任务在多个Executor上运行时的调度问题。该问题为NP-Hard难度的问题，在该论文中，作者使用RL和GNN来解决它。","covers":["http://cdn.leafii.top/img/image-20230115181529685.png","http://cdn.leafii.top/img/image-20230115184822403.png","http://cdn.leafii.top/img/image-20230115190117750.png","http://cdn.leafii.top/img/image-20230115192627762.png","http://cdn.leafii.top/img/image-20230115192642575.png","http://cdn.leafii.top/img/image-20230115192235075.png","http://cdn.leafii.top/img/image-20230115201806948.png","http://cdn.leafii.top/img/image-20230115203338248.png","https://pic2.zhimg.com/v2-83eb2d85ec07441692e6da4f59954399_r.jpg","https://pic2.zhimg.com/80/v2-0d73ffcf998e888ae8ad7d745ccb0591_1440w.webp"],"content":"<h2 id=\"Decima-Learning-Scheduling-Algorithms-for-Data-Processing-Clusters-用强化学习解决调度问题\"><a href=\"#Decima-Learning-Scheduling-Algorithms-for-Data-Processing-Clusters-用强化学习解决调度问题\" class=\"headerlink\" title=\"Decima(Learning Scheduling Algorithms for Data Processing Clusters):用强化学习解决调度问题\"></a>Decima(Learning Scheduling Algorithms for Data Processing Clusters):用强化学习解决调度问题</h2><h3 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h3><p>解决了在云上的多个以DAG图表示的任务在多个Executor上运行时的调度问题。该问题为NP-Hard难度的问题，在该论文中，作者使用RL和GNN来解决它。</p>\n<span id=\"more\"></span>\n\n\n\n<ul>\n<li><strong>任务描述</strong></li>\n</ul>\n<p>每一个节点表示一个计算的阶段(computation stage)， 每个阶段都包含可以并行计算的任务；</p>\n<p>每一条边表示数据依赖(Data dependencies)，父节点完成之后子节点才可以开始运行。</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115181529685.png\" alt=\"image-20230115181529685\" loading=\"lazy\"></p>\n<ul>\n<li><strong>交互过程</strong></li>\n</ul>\n<ol>\n<li>调度器接受任务，并且将挂起的计算阶段映射到可用服务器上运行。</li>\n<li>与此同时不断有新的、随机的任务请求出现，即待分配的任务和可用的服务器都处于动态变化的环境中。</li>\n<li>智能体的观测(Observation)：所有可用作业的信息和服务器在每个调度事件上的状态。(the information of all available jobs and the status of the servers at every scheduling event.)</li>\n<li>目标：最小化任务的平均完成时间。</li>\n</ol>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115184822403.png\" alt=\"image-20230115184822403\" loading=\"lazy\"></p>\n<ul>\n<li>论文贡献：<ul>\n<li>针对具有依赖关系的任务，使用RL训练workload-specific调度算法：Decima</li>\n<li>使用可伸缩的GNN来表示调度策略，可以处理任意形状和大小的DAG表示的任务</li>\n<li>任务随机到达，可以进行在线学习</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"强化学习模型\"><a href=\"#强化学习模型\" class=\"headerlink\" title=\"强化学习模型\"></a>强化学习模型</h3><ul>\n<li>Action组成：(node, 服务器数量)</li>\n<li>当服务器池中有空闲服务器的时候，选择一个node，为其分配相应的节点数量，重复该过程直至所有服务器都处于忙碌状态。这样的<strong>优点</strong>是减少动作空间大小和动作序列的长度，降低强化学习的难度，作者称这是唯一work的设计。</li>\n</ul>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115190117750.png\" alt=\"image-20230115190117750\" loading=\"lazy\"></p>\n<ul>\n<li>State：DAG信息（包括任务数量、平均任务时间、可用服务器数量、待决策的任务数量等</li>\n</ul>\n<h3 id=\"GNN（DAG信息聚合）\"><a href=\"#GNN（DAG信息聚合）\" class=\"headerlink\" title=\"GNN（DAG信息聚合）\"></a>GNN（DAG信息聚合）</h3><p>$e_v &#x3D; g[\\sum_{w\\in\\xi(v)}f(e_w)]+x_v$</p>\n<p>其中x表示节点的特征，w表示所有的子节点，因此节点的打分因子是由子节点分数之和以及节点特征决定的。</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115192627762.png\" alt=\"image-20230115192627762\" loading=\"lazy\"></p>\n<p>使用两个非线性函数是因为它能让Decima表达更加广泛多变的聚合函数，例如$f\\sim log(\\cdot&#x2F;n),g\\sim exp(n \\times \\cdot)$时，Decima能表示下图的关键路径：</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115192642575.png\" alt=\"image-20230115192642575\" loading=\"lazy\"></p>\n<p>$e_t^i &#x3D; g[\\sum_{w\\in \\xi(v)}f(e_w^i)]+x_v^i$</p>\n<p>上式为Per-node embeddings表达式，体现了子节点的信息向当前节点汇聚的过程。与Per-node embedding类似，作者为每个Job和所有的任务都分别设计了Per-job embeddings</p>\n<p>${(x_v^i,e_v^i), v\\in G_i} \\rightarrow y^i$</p>\n<p>以及global embeddings</p>\n<p>${y^1, y^2, …} \\rightarrow z$</p>\n<p>GNN(节点信息聚合)的全过程在论文中由下图表示：</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115192235075.png\" loading=\"lazy\"></p>\n<p>因此，每个层级的信息聚合由两个非线性变换(f and g)表示；整个GNN由6个非线性变换构成。</p>\n<h3 id=\"决策网络设计\"><a href=\"#决策网络设计\" class=\"headerlink\" title=\"决策网络设计\"></a>决策网络设计</h3><ul>\n<li>调度过程：当某个job完成或者有新job到达时，系统开始调度。</li>\n<li>动作(action)$&lt;v,l_i&gt;$有两个维度，决策网络的输出包括两部分，v是选择哪个任务节点($P_{node}$)，$l_i$是为这个任务分配多少计算资源(Parallelism limit on job)。调度包括以下三个阶段：<ul>\n<li>当任务i使用的服务器比$l_i$少时，Decima为节点$v$分配executors直到executors的数量到达$l_i$.</li>\n<li>上一步完成后，如果有多余的executors，Deciam将继续运行agent得到stage($v$)和parallelism($l_i$).</li>\n<li>上述步骤一直运行，直至没有空闲的executors和未处理的stage($v$).</li>\n</ul>\n</li>\n</ul>\n<p>网络结构如下图所示，包括了GNN部分以及决策网络部分，GNN负责聚合DAG的信息，决策网络负责输出stage($v$)以及并行度上线(parallelism limit)($l_i$):</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115201806948.png\" alt=\"image-20230115201806948\" loading=\"lazy\"></p>\n<h3 id=\"决策网络运行过程\"><a href=\"#决策网络运行过程\" class=\"headerlink\" title=\"决策网络运行过程\"></a>决策网络运行过程</h3><ol>\n<li>得到stage(v)</li>\n</ol>\n<ul>\n<li>用非线形网络$q(\\cdot)$，得到$q_v^i\\triangleq q(e_v^i, y^i, z)$</li>\n<li>使用softmax层，选择调度的stage(v)</li>\n</ul>\n<p>$P(node &#x3D; v) &#x3D; \\frac{exp(q_v^i)}{\\sum_{u\\in A_t}exp(q_u^{j(u)})}$</p>\n<ol start=\"2\">\n<li>为任务节点分配服务器数量parallelism limit($l_i$)</li>\n</ol>\n<p>计算过程与得到stage(v)类似</p>\n<h3 id=\"训练模型与实验\"><a href=\"#训练模型与实验\" class=\"headerlink\" title=\"训练模型与实验\"></a>训练模型与实验</h3><p>系统的整体结构图如下：</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115203338248.png\" alt=\"image-20230115203338248\" loading=\"lazy\"></p>\n<p>模型训练采用的是策略梯度法(PG):</p>\n<p><img src=\"https://pic2.zhimg.com/v2-83eb2d85ec07441692e6da4f59954399_r.jpg\" alt=\"img\" loading=\"lazy\"></p>\n<p><img src=\"https://pic2.zhimg.com/80/v2-0d73ffcf998e888ae8ad7d745ccb0591_1440w.webp\" alt=\"img\" loading=\"lazy\"></p>\n<p>作者在训练时采用以下公式更新策略梯度：</p>\n<p>$\\theta \\leftarrow \\theta + \\alpha \\sum_{k&#x3D;1}^{T} \\nabla_\\theta log \\pi_\\theta (s_k, a_k)(\\sum_{k’ &#x3D; k}^{T}r_{k’} - b_k)$</p>\n<p>其中$\\alpha$为学习率， $b_k$为baseline，添加baseline是参见此<a href=\"https://leafii.top/2022/09/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0_PPO%E7%AE%97%E6%B3%95(Proximal%20Policy%20Optimization)/\">链接</a></p>\n<p>在训练初期使用较短的任务序列，逐渐加长序列。</p>\n<ul>\n<li>因为在训练初期，agent比较笨，它无法对长序列进行很好的处理，反而会导致任务堆积，学习效率也很低。因此作者的构想为先处理简单的，再逐渐增加难度，效率会高一些。</li>\n<li>这是一种课程学习(curriculum learning)的模式</li>\n</ul>\n<p><strong>与(朴素的)传统方法比较</strong></p>\n<ul>\n<li>朴素的先进任务先执行、最短任务优先会导致资源闲置等待。朴素的公平分配虽然会使服务器没有闲置，但是会导致大量IO开销从而拖慢速度。而<strong>Decima</strong></li>\n<li>有选择的将服务器集中在某些小任务上，同时仍然不留下任何资源空闲。</li>\n<li>为每个作业分配适当数量的服务器，这样就不会产生不必要的IO工作。</li>\n</ul>\n<p><strong>实验</strong></p>\n<ul>\n<li>在任务batched arrivals、按泊松分布continuous arrivals两个实验里，Decima完爆传统方法。</li>\n<li>ppt里暂时没看到对任务信息的更进一步描述，比如复杂度分布之类的。不过讲到了Decima完成小任务特别快，并且倾向于对小任务分配更多（但不会过多）的服务器。</li>\n<li>其他实验：解决其他资源调度问题、评估算法各个部分的影响、大规模集群泛化、训练及推理速度、离线优化等，ppt里没讲，等我看了论文再补充</li>\n</ul>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p><a href=\"https://zhuanlan.zhihu.com/p/133427987\">RL for Scheduling，如何用强化学习解决调度问题？ - 知乎 (zhihu.com)</a></p>\n<p><a href=\"https://link.zhihu.com/?target=https://web.mit.edu/decima/\">Decima (mit.edu)</a></p>\n<p><a href=\"https://link.zhihu.com/?target=https://www.bilibili.com/video/BV1MW411w79n?spm_id_from=333.999.0.0\">李宏毅深度强化学习(国语)课程(2018)_哔哩哔哩_bilibili</a></p>\n","more":"<ul>\n<li><strong>任务描述</strong></li>\n</ul>\n<p>每一个节点表示一个计算的阶段(computation stage)， 每个阶段都包含可以并行计算的任务；</p>\n<p>每一条边表示数据依赖(Data dependencies)，父节点完成之后子节点才可以开始运行。</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115181529685.png\" alt=\"image-20230115181529685\"></p>\n<ul>\n<li><strong>交互过程</strong></li>\n</ul>\n<ol>\n<li>调度器接受任务，并且将挂起的计算阶段映射到可用服务器上运行。</li>\n<li>与此同时不断有新的、随机的任务请求出现，即待分配的任务和可用的服务器都处于动态变化的环境中。</li>\n<li>智能体的观测(Observation)：所有可用作业的信息和服务器在每个调度事件上的状态。(the information of all available jobs and the status of the servers at every scheduling event.)</li>\n<li>目标：最小化任务的平均完成时间。</li>\n</ol>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115184822403.png\" alt=\"image-20230115184822403\"></p>\n<ul>\n<li>论文贡献：<ul>\n<li>针对具有依赖关系的任务，使用RL训练workload-specific调度算法：Decima</li>\n<li>使用可伸缩的GNN来表示调度策略，可以处理任意形状和大小的DAG表示的任务</li>\n<li>任务随机到达，可以进行在线学习</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"强化学习模型\"><a href=\"#强化学习模型\" class=\"headerlink\" title=\"强化学习模型\"></a>强化学习模型</h3><ul>\n<li>Action组成：(node, 服务器数量)</li>\n<li>当服务器池中有空闲服务器的时候，选择一个node，为其分配相应的节点数量，重复该过程直至所有服务器都处于忙碌状态。这样的<strong>优点</strong>是减少动作空间大小和动作序列的长度，降低强化学习的难度，作者称这是唯一work的设计。</li>\n</ul>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115190117750.png\" alt=\"image-20230115190117750\"></p>\n<ul>\n<li>State：DAG信息（包括任务数量、平均任务时间、可用服务器数量、待决策的任务数量等</li>\n</ul>\n<h3 id=\"GNN（DAG信息聚合）\"><a href=\"#GNN（DAG信息聚合）\" class=\"headerlink\" title=\"GNN（DAG信息聚合）\"></a>GNN（DAG信息聚合）</h3><p>$e_v &#x3D; g[\\sum_{w\\in\\xi(v)}f(e_w)]+x_v$</p>\n<p>其中x表示节点的特征，w表示所有的子节点，因此节点的打分因子是由子节点分数之和以及节点特征决定的。</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115192627762.png\" alt=\"image-20230115192627762\"></p>\n<p>使用两个非线性函数是因为它能让Decima表达更加广泛多变的聚合函数，例如$f\\sim log(\\cdot&#x2F;n),g\\sim exp(n \\times \\cdot)$时，Decima能表示下图的关键路径：</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115192642575.png\" alt=\"image-20230115192642575\"></p>\n<p>$e_t^i &#x3D; g[\\sum_{w\\in \\xi(v)}f(e_w^i)]+x_v^i$</p>\n<p>上式为Per-node embeddings表达式，体现了子节点的信息向当前节点汇聚的过程。与Per-node embedding类似，作者为每个Job和所有的任务都分别设计了Per-job embeddings</p>\n<p>${(x_v^i,e_v^i), v\\in G_i} \\rightarrow y^i$</p>\n<p>以及global embeddings</p>\n<p>${y^1, y^2, …} \\rightarrow z$</p>\n<p>GNN(节点信息聚合)的全过程在论文中由下图表示：</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115192235075.png\"></p>\n<p>因此，每个层级的信息聚合由两个非线性变换(f and g)表示；整个GNN由6个非线性变换构成。</p>\n<h3 id=\"决策网络设计\"><a href=\"#决策网络设计\" class=\"headerlink\" title=\"决策网络设计\"></a>决策网络设计</h3><ul>\n<li>调度过程：当某个job完成或者有新job到达时，系统开始调度。</li>\n<li>动作(action)$&lt;v,l_i&gt;$有两个维度，决策网络的输出包括两部分，v是选择哪个任务节点($P_{node}$)，$l_i$是为这个任务分配多少计算资源(Parallelism limit on job)。调度包括以下三个阶段：<ul>\n<li>当任务i使用的服务器比$l_i$少时，Decima为节点$v$分配executors直到executors的数量到达$l_i$.</li>\n<li>上一步完成后，如果有多余的executors，Deciam将继续运行agent得到stage($v$)和parallelism($l_i$).</li>\n<li>上述步骤一直运行，直至没有空闲的executors和未处理的stage($v$).</li>\n</ul>\n</li>\n</ul>\n<p>网络结构如下图所示，包括了GNN部分以及决策网络部分，GNN负责聚合DAG的信息，决策网络负责输出stage($v$)以及并行度上线(parallelism limit)($l_i$):</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115201806948.png\" alt=\"image-20230115201806948\"></p>\n<h3 id=\"决策网络运行过程\"><a href=\"#决策网络运行过程\" class=\"headerlink\" title=\"决策网络运行过程\"></a>决策网络运行过程</h3><ol>\n<li>得到stage(v)</li>\n</ol>\n<ul>\n<li>用非线形网络$q(\\cdot)$，得到$q_v^i\\triangleq q(e_v^i, y^i, z)$</li>\n<li>使用softmax层，选择调度的stage(v)</li>\n</ul>\n<p>$P(node &#x3D; v) &#x3D; \\frac{exp(q_v^i)}{\\sum_{u\\in A_t}exp(q_u^{j(u)})}$</p>\n<ol start=\"2\">\n<li>为任务节点分配服务器数量parallelism limit($l_i$)</li>\n</ol>\n<p>计算过程与得到stage(v)类似</p>\n<h3 id=\"训练模型与实验\"><a href=\"#训练模型与实验\" class=\"headerlink\" title=\"训练模型与实验\"></a>训练模型与实验</h3><p>系统的整体结构图如下：</p>\n<p><img src=\"http://cdn.leafii.top/img/image-20230115203338248.png\" alt=\"image-20230115203338248\"></p>\n<p>模型训练采用的是策略梯度法(PG):</p>\n<p><img src=\"https://pic2.zhimg.com/v2-83eb2d85ec07441692e6da4f59954399_r.jpg\" alt=\"img\"></p>\n<p><img src=\"https://pic2.zhimg.com/80/v2-0d73ffcf998e888ae8ad7d745ccb0591_1440w.webp\" alt=\"img\"></p>\n<p>作者在训练时采用以下公式更新策略梯度：</p>\n<p>$\\theta \\leftarrow \\theta + \\alpha \\sum_{k&#x3D;1}^{T} \\nabla_\\theta log \\pi_\\theta (s_k, a_k)(\\sum_{k’ &#x3D; k}^{T}r_{k’} - b_k)$</p>\n<p>其中$\\alpha$为学习率， $b_k$为baseline，添加baseline是参见此<a href=\"https://leafii.top/2022/09/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0_PPO%E7%AE%97%E6%B3%95(Proximal%20Policy%20Optimization)/\">链接</a></p>\n<p>在训练初期使用较短的任务序列，逐渐加长序列。</p>\n<ul>\n<li>因为在训练初期，agent比较笨，它无法对长序列进行很好的处理，反而会导致任务堆积，学习效率也很低。因此作者的构想为先处理简单的，再逐渐增加难度，效率会高一些。</li>\n<li>这是一种课程学习(curriculum learning)的模式</li>\n</ul>\n<p><strong>与(朴素的)传统方法比较</strong></p>\n<ul>\n<li>朴素的先进任务先执行、最短任务优先会导致资源闲置等待。朴素的公平分配虽然会使服务器没有闲置，但是会导致大量IO开销从而拖慢速度。而<strong>Decima</strong></li>\n<li>有选择的将服务器集中在某些小任务上，同时仍然不留下任何资源空闲。</li>\n<li>为每个作业分配适当数量的服务器，这样就不会产生不必要的IO工作。</li>\n</ul>\n<p><strong>实验</strong></p>\n<ul>\n<li>在任务batched arrivals、按泊松分布continuous arrivals两个实验里，Decima完爆传统方法。</li>\n<li>ppt里暂时没看到对任务信息的更进一步描述，比如复杂度分布之类的。不过讲到了Decima完成小任务特别快，并且倾向于对小任务分配更多（但不会过多）的服务器。</li>\n<li>其他实验：解决其他资源调度问题、评估算法各个部分的影响、大规模集群泛化、训练及推理速度、离线优化等，ppt里没讲，等我看了论文再补充</li>\n</ul>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p><a href=\"https://zhuanlan.zhihu.com/p/133427987\">RL for Scheduling，如何用强化学习解决调度问题？ - 知乎 (zhihu.com)</a></p>\n<p><a href=\"https://link.zhihu.com/?target=https://web.mit.edu/decima/\">Decima (mit.edu)</a></p>\n<p><a href=\"https://link.zhihu.com/?target=https://www.bilibili.com/video/BV1MW411w79n?spm_id_from=333.999.0.0\">李宏毅深度强化学习(国语)课程(2018)_哔哩哔哩_bilibili</a></p>","categories":[],"tags":[{"name":"python","path":"api/tags/python.json"},{"name":"强化学习","path":"api/tags/强化学习.json"}]}