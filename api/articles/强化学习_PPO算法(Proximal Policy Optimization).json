{"title":"强化学习_PPO算法(Proximal Policy Optimization)","slug":"强化学习_PPO算法(Proximal Policy Optimization)","date":"2022-09-20T13:15:18.000Z","updated":"2022-09-23T12:02:15.613Z","comments":true,"path":"api/articles/强化学习_PPO算法(Proximal Policy Optimization).json","excerpt":"本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。","covers":["http://cdn.leafii.top/img/secondppo.png"],"content":"<p>本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。</p>\n<span id=\"more\"></span>\n\n<p>PPO(Proximal Policy Optimization)是OpenAI使用的默认RL方法，PPO方法可以被理解为</p>\n<p><code>Policy Gradient -&gt; (On Policy -&gt; Off Policy) -&gt; (Add Constraint) -&gt; PPO(Proximal Policy Optimization)</code></p>\n<h2 id=\"RL相关要素\"><a href=\"#RL相关要素\" class=\"headerlink\" title=\"RL相关要素\"></a>RL相关要素</h2><p>强化学习是指智能体在给定环境中进行动作的选择，在动作选择并执行之后同环境交互获得新的状态，每一对State（状态）和Action（动作）可以得到相应的Reward（奖励），强化学习的目标就是最大化Reward。</p>\n<p>状态，动作更替可以用Trajectory（迹）来表示：</p>\n<p>$$Trajectory\\ \\tau &#x3D; {s_1,a_1,s_2,a_2,…,s_T,a_T}\\tag{1}$$</p>\n<p>每一条Trajectory的概率为:<br>$$<br>p_\\theta(\\tau) &amp;&#x3D; p(s_1)p_\\theta(a_1|s_1)p(s_2|s_1,a_1)p_\\theta(a_2|s_2)p(s_3|s_2,a_2)…<br>\\&amp;&#x3D;p(s_1)\\prod_{t&#x3D;1}^{T}p_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\\tag{2}<br>$$<br>在这个式子中我们可以看到，$p_\\theta(a_t|s_t)$是Actor得出的，这个是我们可以控制的，但是$p(s_{t+1}|s_t,a_t)$是动作$a_t$在状态$s_t$下与环境交互转移到状态$s_{t+1}$的概率，这是由环境本身决定的，我们自己无法控制它。</p>\n<p>在强化学习中一条Trajectory的Expected Reward为：<br>$$<br>\\overline R_\\theta &#x3D; \\sum_{\\tau}R(\\tau)p_\\theta(\\tau)&#x3D;E_{\\tau\\sim p_\\theta(\\tau)}[R(\\tau)]\\tag{3})<br>$$<br>其中的$R(\\tau)$为：<br>$$<br>R(\\tau) &#x3D; \\sum_{t&#x3D;1}^{T} r_t\\tag{4}<br>$$</p>\n<h2 id=\"Policy-Gradient\"><a href=\"#Policy-Gradient\" class=\"headerlink\" title=\"Policy Gradient\"></a>Policy Gradient</h2><p>想要最大化Reward，在上一节我们又获得了$\\overline R_\\theta$的公式，此时我们对$\\overline R_\\theta$求梯度:<br>$$<br>\\nabla \\overline R_\\theta &#x3D; \\sum_{\\tau}R(\\tau)\\nabla p_\\theta(\\tau)&#x3D;\\sum_{\\tau}R(\\tau)p_\\theta(\\tau)\\frac{\\nabla p_\\theta(\\tau)}{p_\\theta(\\tau)}\\tag{5}<br>$$<br>在上式中，$R(\\tau)$不一定是需要可微的，它甚至可以是一个黑盒。<br>$$<br>\\nabla f(x) &#x3D; f(x)\\nabla logf(x)\\tag{6}<br>$$<br>接着将梯度公式(6)代入(5)中，得：<br>$$<br>\\nabla \\overline R_\\theta &amp;&#x3D; \\sum_{\\tau}R(\\tau)p_\\theta(\\tau)\\nabla logp_\\theta(\\tau)<br>\\&amp;&#x3D; E_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla logp_\\theta(\\tau)]<br>\\&amp; \\approx \\frac{1}{N}\\sum_{n&#x3D;1}^{N}R(\\tau^n)\\nabla logp_\\theta(\\tau^n)<br>\\ &amp;&#x3D; \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}R(\\tau^n)\\nabla logp_\\theta(a_t^n|s_t^n)\\tag{7}<br>$$<br>在式子(7)中，由于$\\sum_{\\tau}$和$p_{\\theta}(\\tau)$的存在，因此将它们写成期望$E_{\\tau \\sim p_\\theta(\\tau)}$的形式。$p_{\\theta}(\\tau)$相当于$\\nabla logp_\\theta(\\tau)$的一个weight（权重）。并且在$p_\\theta(\\tau)$中，$\\tau$相当于有两项，一项是来自环境本身（无法求梯度），另一项来自智能体Agent，因此将$\\nabla logp_\\theta(\\tau)$更进一步写作$logp_\\theta(a_t^n|s_t^n)$</p>\n<p>因此的Policy Gradient的基本过程可以这样描述：在环境中取得数据，在给定的策略$\\pi_\\theta$下，获得不同的Trajectory，每个Trajectory拥有不同的状态，动作以及对应的奖励值，收集状态动作对之后，将其带入式(7)中，进行参数$\\theta$的更新：$\\theta \\leftarrow \\theta + \\eta\\nabla \\overline R_\\theta$,在更新之后对新的环境重新获取数据，循环往复，我们可以看到在这个流程中，在环境中取得的数据仅被使用了一次，因此Policy Gradient是一个严格的On-Policy算法。</p>\n<h3 id=\"Tip-1-Add-a-Baseline\"><a href=\"#Tip-1-Add-a-Baseline\" class=\"headerlink\" title=\"Tip 1: Add a Baseline\"></a>Tip 1: Add a Baseline</h3><p>在某些情况下，$\\theta \\leftarrow \\theta + \\eta\\nabla \\overline R_\\theta$中的$R(\\tau^n)$始终为正，如果采样数量足够多，即使是奖励都为正的动作，我们也可以按照奖励值的大小决定每个动作的优劣，并以此修改下个动作被选中的概率（好的动作增加被选中的概率，差的动作降低被选中的概率），但是在训练的环境下，总会有一些奖励为正的动作无法被采样，但是其他动作的奖励都为正，它们被选中的概率增加好，这就导致未被采样的动作的概率降低，哪怕未被采样的动作实质上优于一部分甚至全部被采样的动作。因此可以将式(7)进行如下修改：<br>$$<br>\\nabla \\overline R_\\theta &#x3D; \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}(R(\\tau^n)-b)\\nabla logp_\\theta(a_t^n|s_t^n)\\tag{8}<br>$$<br>在这里的b就是新增的baseline，通常$b \\approx E[R(\\tau)]$，这样就可以有效的降低未被采样的动作被“误杀”。</p>\n<h3 id=\"Tip-2-Assign-Suitable-Credit\"><a href=\"#Tip-2-Assign-Suitable-Credit\" class=\"headerlink\" title=\"Tip 2: Assign Suitable Credit\"></a>Tip 2: Assign Suitable Credit</h3><p>在一条Trajectory中，每个动作如果只由总的R来反映权重是不合适的，比如在$(s_a,a_1),(s_b,a_2),(s_c,a_3)$中的单步奖励值分别为+5，+0，-2，R&#x3D;5-2&#x3D;+3 但是对于a2，a3这种并未对Reward结果最大化做出正向贡献的action反而也被赋予了值为+3的Reward作为权重，这是不合理的。我们应该让每一个action前的R值都正确的反映它在当前Trajectory中的作用，到底是好还是坏。我们可以把式(8)中的$R(\\tau^n)$改写为$\\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$:<br>$$<br>\\nabla \\overline R_\\theta \\approx \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}(\\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}-b)\\nabla p_\\theta(a_t^n|s_t^n)\\tag{9}<br>$$<br>$\\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$又可以进一步写为$\\sum_{t’&#x3D;t}^{T_n}\\gamma^{t’-t} r_{t’}^{n}$，其中$\\gamma$作为discount factor(折扣因子)并且$\\gamma &lt; 1$，因此可以得到：<br>$$<br>\\nabla \\overline R_\\theta \\approx \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}(\\sum_{t’&#x3D;t}^{T_n}\\gamma^{t’-t} r_{t’}^{n}-b)\\nabla p_\\theta(a_t^n|s_t^n)\\tag{10}<br>$$</p>\n<h2 id=\"On-policy-v-s-Off-policy\"><a href=\"#On-policy-v-s-Off-policy\" class=\"headerlink\" title=\"On-policy v.s. Off-policy\"></a>On-policy v.s. Off-policy</h2><ul>\n<li>On-policy：agent学习与交互的环境是相同的。</li>\n<li>Off-policy：agent学习的环境和交互的环境并不相同。</li>\n</ul>\n<h3 id=\"从On-policy转向Off-policy的分析\"><a href=\"#从On-policy转向Off-policy的分析\" class=\"headerlink\" title=\"从On- policy转向Off-policy的分析\"></a>从On- policy转向Off-policy的分析</h3><p>On-policy的情况下，Policy Gradient的公式为:<br>$$<br>\\nabla \\overline R_\\theta &#x3D; E_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla logp_\\theta(\\tau)]\\tag{11}<br>$$<br>在On-policy中，我们使用$\\pi_{\\theta}$去收集数据，当$\\theta$被更新的时候，我们必须去重新采样训练数据。</p>\n<p>我们现在的目标是：使用$\\pi_{\\theta’}$采样获得数据去训练$\\theta$。$\\theta’$是一个固定的值，因此我们可以重复利用采样的数据。</p>\n<h3 id=\"重要性采样\"><a href=\"#重要性采样\" class=\"headerlink\" title=\"重要性采样\"></a>重要性采样</h3><p>如果正常从p中进行采样获得x的期望值：<br>$$<br>E_{x\\sim p}[f(x)] \\approx \\frac{1}{N}\\sum_{i&#x3D;1}^{N}f(x^i)\\tag{12}<br>$$<br>但是现在我们不从p中采样，只能从q中采样呢？<br>$$<br>E_{x\\sim p}[f(x)] &amp;\\approx \\frac{1}{N}\\sum_{i&#x3D;1}^{N}f(x^i)<br>\\&amp;&#x3D;\\int f(x)p(x)dx \\&amp;&#x3D; \\int f(x) \\frac{p(x)}{q(x)}q(x)dx \\&amp;&#x3D; E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\\tag{13}<br>$$<br>通过这样的变换，我们就达到了从q中采样获取x期望值的效果。</p>\n<p>重要性采样公式为：<br>$$<br>E_{x\\sim p}[f(x)]&#x3D; E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\\tag{14}<br>$$</p>\n<h3 id=\"On-policy-gt-Off-policy\"><a href=\"#On-policy-gt-Off-policy\" class=\"headerlink\" title=\"On-policy-&gt;Off-policy\"></a>On-policy-&gt;Off-policy</h3><p>$$<br>\\nabla \\overline R_\\theta &amp;&#x3D; E_{(s_t,a_t)\\sim \\pi_\\theta}[A^\\theta(s_t,a_t)\\nabla logp_\\theta(a_t^n|s_t^n)]<br>\\ &amp;&#x3D;E_{(s_t,a_t)\\sim \\pi_\\theta’}[\\frac{P_\\theta(s_t,a_t)}{P_\\theta’(s_t,a_t)}A^{\\theta’}(s_t,a_t)\\nabla logp_\\theta(a_t^n|s_t^n)]<br>\\ &amp;&#x3D;E_{(s_t,a_t)\\sim \\pi_\\theta’}[\\frac{p_\\theta(a_t|s_t)}{p_\\theta’(a_t|s_t)}\\frac{p_\\theta(s_t)}{p_\\theta’(s_t)}A^{\\theta’}(s_t,a_t)\\nabla logp_\\theta(a_t^n|s_t^n)]\\tag{15}<br>$$</p>\n<p>由于我们假设$\\theta$与$\\theta’$是一样的，因此${p_\\theta(s_t)}$和 $ p_\\theta’(s_t)$这两个同环境相关的参数可以约掉，而${p_\\theta(a_t|s_t)}$和${p_\\theta’(a_t|s_t)}$是同动作选择相关，并没有假设它们的动作选择一致，因此不能约掉。因此我们得到：<br>$$<br>J^{\\theta’}(\\theta) &#x3D; E_{(s_t,a_t)\\sim \\pi_{\\theta’}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta’}(a_t|s_t)}A^{\\theta’}(s_t,a_t)]\\tag{16}<br>$$</p>\n<h3 id=\"Add-Constraint\"><a href=\"#Add-Constraint\" class=\"headerlink\" title=\"Add Constraint\"></a>Add Constraint</h3><p>$\\theta$和$\\theta’$的区别是一个值得讨论的问题。在这里我们所说的区别并不是$\\theta$和$\\theta’$参数上的不同，而是说它们在表现上的不同的程度需要被限制，而想要这个区别被限制，就必须要使它可以被量化。因此，在PPO中，使用$KL(\\theta,\\theta’)$对$\\theta$和$\\theta’$在表现上的不同的程度进行量化。由此可以得到两个算法，即Proximal Policy Optimization(PPO)和TRPO(Trust Region Policy Optimization)：</p>\n<ul>\n<li>Proximal Policy Optimization(PPO)</li>\n</ul>\n<p>$$<br>J_{PPO}^{\\theta’}(\\theta) &#x3D; J^{\\theta’}(\\theta) - \\beta KL(\\theta,\\theta’)\\tag{17}<br>$$</p>\n<ul>\n<li>TRPO(Trust Region Policy Optimization)</li>\n</ul>\n<p>$$<br>J_{TRPO}^{\\theta’}(\\theta) &#x3D; E_{(s_t,a_t)\\sim \\pi_{\\theta’}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta’}(a_t|s_t)}A^{\\theta’}(s_t,a_t)]\\tag{18}<br>$$</p>\n<p>在TRPO中，$KL(\\theta,\\theta’)$以单独的限制存在，一般为$KL(\\theta,\\theta’)&lt;\\delta$.</p>\n<p>因此，PPO算法的伪代码如下：</p>\n<ul>\n<li><p>Initial policy parameters $\\theta^0$</p>\n</li>\n<li><p>In each iteration:</p>\n<ul>\n<li>Using $\\theta^k$ to interact with the environment to collect ${s_t,a_t}$ and compute advatage $A_{\\theta^k}(st,at)$</li>\n<li>Find $\\theta$ optimizing $J_{PPO}(\\theta)$</li>\n<li>$J_{PPO}^{\\theta^k} &#x3D; J^{\\theta^k}(\\theta) - \\beta \\times KL(\\theta,\\theta^k)$ # Update parameters several times</li>\n</ul>\n</li>\n<li><p>与此同时，动态调整$\\beta$:</p>\n<ul>\n<li>If $KL(\\theta,\\theta^k) &gt; KL_{max}$,increase $\\beta$</li>\n<li>If $KL(\\theta,\\theta^k) &lt; KL_{min}$,decrease $\\beta$</li>\n</ul>\n</li>\n</ul>\n<p>这就完成了 KL Penalty的建立。</p>\n<h2 id=\"第二种PPO算法\"><a href=\"#第二种PPO算法\" class=\"headerlink\" title=\"第二种PPO算法\"></a>第二种PPO算法</h2><p>在这个PPO算法中，并不是使用KL函数，而是使用clip函数，所谓的clip函数是指$clip(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)},1-\\epsilon,1+\\epsilon)$，当$\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)}$的值小于$1-\\epsilon$时，它的值取$1-\\epsilon$,当$\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)}$的值大于$1+\\epsilon$时，它的值取$1+\\epsilon$.因此可得：<br>$$<br>J_{PPO2}^{\\theta^k}(\\theta) \\approx \\sum_{(s_t,a_t)}min(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t|s_t)}A^{\\theta^k}(s_t,a_t),clip(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)},1-\\epsilon,1+\\epsilon)A^{\\theta^k}(s_t,a_t))<br>$$<br>$\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t|s_t)}$和$clip(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)},1-\\epsilon,1+\\epsilon)$的图像如图所示：</p>\n<p><img src=\"http://cdn.leafii.top/img/secondppo.png\" alt=\"secondppo\" loading=\"lazy\"></p>\n<h2 id=\"PPO的网络结构\"><a href=\"#PPO的网络结构\" class=\"headerlink\" title=\"PPO的网络结构\"></a>PPO的网络结构</h2><p>未完待续。。。</p>\n<h2 id=\"为什么说TRPO和PPO是On-policy的？\"><a href=\"#为什么说TRPO和PPO是On-policy的？\" class=\"headerlink\" title=\"为什么说TRPO和PPO是On-policy的？\"></a>为什么说TRPO和PPO是On-policy的？</h2><p><strong>首先我们明确什么是on-policy，什么是off-policy？</strong></p>\n<ul>\n<li>on-policy：就是要训练的agent跟环境互动的agent是同一个agent,也就是我们采样的网络和要优化的网络是否是同一个网络。</li>\n<li>off-policy：那肯定就是跟上面相反的。</li>\n</ul>\n<p>那么进入正题，我们一般认为PPO是off-policy的原因就是因为PPO使用actor网络去sampler然后填充经验池，然后使用这个经验池中的数据去更新这个actor多个epoch，当更新到第二个epoch的时候那么actor就变成了actor1,然而经验池中的数据仍然是actor网络采样得到的，那么就造成了从更新第二个epoch开始采样的actor和要优化的actor不是同一个网络，那么可能就会认为它是off-pocliy的。</p>\n<p>其实可以很简单的解释这个问题，根据off-policy的定义，采样的网络和要优化的网络不是一个网络，那么对于PPO来说，<strong>使用一批数据从更新actor的第二个epoch开始，数据虽然都是旧的actor采样得到的，但是我们并没有直接使用这批数据去更新我们的新的actor，而是使用imporance sampling先将数据分布不同导致的误差进行了修正</strong>。那么这个importance sampling的目的就是让这两者数据分布之间的差异尽可能的缩小，<strong>那么就可以近似理解成做了importance sampling之后的数据就是我们的更新（这里的更新指的是多个epoch更新的中间过程）后的actor采样得来的，这样就可以理解成我们要优化得actor和采样得actor是同一个actor，那么他就是on-policy的。</strong></p>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ol>\n<li><p>【DRL-16】Proximal Policy Optimization : <a href=\"https://zhuanlan.zhihu.com/p/142312072\">https://zhuanlan.zhihu.com/p/142312072</a></p>\n</li>\n<li><p>为什么说TRPO和PPO是on-policy的？:<a href=\"https://zhuanlan.zhihu.com/p/387193698\">https://zhuanlan.zhihu.com/p/387193698</a></p>\n</li>\n</ol>\n","more":"<p>PPO(Proximal Policy Optimization)是OpenAI使用的默认RL方法，PPO方法可以被理解为</p>\n<p><code>Policy Gradient -&gt; (On Policy -&gt; Off Policy) -&gt; (Add Constraint) -&gt; PPO(Proximal Policy Optimization)</code></p>\n<h2 id=\"RL相关要素\"><a href=\"#RL相关要素\" class=\"headerlink\" title=\"RL相关要素\"></a>RL相关要素</h2><p>强化学习是指智能体在给定环境中进行动作的选择，在动作选择并执行之后同环境交互获得新的状态，每一对State（状态）和Action（动作）可以得到相应的Reward（奖励），强化学习的目标就是最大化Reward。</p>\n<p>状态，动作更替可以用Trajectory（迹）来表示：</p>\n<p>$$Trajectory\\ \\tau &#x3D; {s_1,a_1,s_2,a_2,…,s_T,a_T}\\tag{1}$$</p>\n<p>每一条Trajectory的概率为:<br>$$<br>p_\\theta(\\tau) &amp;&#x3D; p(s_1)p_\\theta(a_1|s_1)p(s_2|s_1,a_1)p_\\theta(a_2|s_2)p(s_3|s_2,a_2)…<br>\\&amp;&#x3D;p(s_1)\\prod_{t&#x3D;1}^{T}p_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\\tag{2}<br>$$<br>在这个式子中我们可以看到，$p_\\theta(a_t|s_t)$是Actor得出的，这个是我们可以控制的，但是$p(s_{t+1}|s_t,a_t)$是动作$a_t$在状态$s_t$下与环境交互转移到状态$s_{t+1}$的概率，这是由环境本身决定的，我们自己无法控制它。</p>\n<p>在强化学习中一条Trajectory的Expected Reward为：<br>$$<br>\\overline R_\\theta &#x3D; \\sum_{\\tau}R(\\tau)p_\\theta(\\tau)&#x3D;E_{\\tau\\sim p_\\theta(\\tau)}[R(\\tau)]\\tag{3})<br>$$<br>其中的$R(\\tau)$为：<br>$$<br>R(\\tau) &#x3D; \\sum_{t&#x3D;1}^{T} r_t\\tag{4}<br>$$</p>\n<h2 id=\"Policy-Gradient\"><a href=\"#Policy-Gradient\" class=\"headerlink\" title=\"Policy Gradient\"></a>Policy Gradient</h2><p>想要最大化Reward，在上一节我们又获得了$\\overline R_\\theta$的公式，此时我们对$\\overline R_\\theta$求梯度:<br>$$<br>\\nabla \\overline R_\\theta &#x3D; \\sum_{\\tau}R(\\tau)\\nabla p_\\theta(\\tau)&#x3D;\\sum_{\\tau}R(\\tau)p_\\theta(\\tau)\\frac{\\nabla p_\\theta(\\tau)}{p_\\theta(\\tau)}\\tag{5}<br>$$<br>在上式中，$R(\\tau)$不一定是需要可微的，它甚至可以是一个黑盒。<br>$$<br>\\nabla f(x) &#x3D; f(x)\\nabla logf(x)\\tag{6}<br>$$<br>接着将梯度公式(6)代入(5)中，得：<br>$$<br>\\nabla \\overline R_\\theta &amp;&#x3D; \\sum_{\\tau}R(\\tau)p_\\theta(\\tau)\\nabla logp_\\theta(\\tau)<br>\\&amp;&#x3D; E_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla logp_\\theta(\\tau)]<br>\\&amp; \\approx \\frac{1}{N}\\sum_{n&#x3D;1}^{N}R(\\tau^n)\\nabla logp_\\theta(\\tau^n)<br>\\ &amp;&#x3D; \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}R(\\tau^n)\\nabla logp_\\theta(a_t^n|s_t^n)\\tag{7}<br>$$<br>在式子(7)中，由于$\\sum_{\\tau}$和$p_{\\theta}(\\tau)$的存在，因此将它们写成期望$E_{\\tau \\sim p_\\theta(\\tau)}$的形式。$p_{\\theta}(\\tau)$相当于$\\nabla logp_\\theta(\\tau)$的一个weight（权重）。并且在$p_\\theta(\\tau)$中，$\\tau$相当于有两项，一项是来自环境本身（无法求梯度），另一项来自智能体Agent，因此将$\\nabla logp_\\theta(\\tau)$更进一步写作$logp_\\theta(a_t^n|s_t^n)$</p>\n<p>因此的Policy Gradient的基本过程可以这样描述：在环境中取得数据，在给定的策略$\\pi_\\theta$下，获得不同的Trajectory，每个Trajectory拥有不同的状态，动作以及对应的奖励值，收集状态动作对之后，将其带入式(7)中，进行参数$\\theta$的更新：$\\theta \\leftarrow \\theta + \\eta\\nabla \\overline R_\\theta$,在更新之后对新的环境重新获取数据，循环往复，我们可以看到在这个流程中，在环境中取得的数据仅被使用了一次，因此Policy Gradient是一个严格的On-Policy算法。</p>\n<h3 id=\"Tip-1-Add-a-Baseline\"><a href=\"#Tip-1-Add-a-Baseline\" class=\"headerlink\" title=\"Tip 1: Add a Baseline\"></a>Tip 1: Add a Baseline</h3><p>在某些情况下，$\\theta \\leftarrow \\theta + \\eta\\nabla \\overline R_\\theta$中的$R(\\tau^n)$始终为正，如果采样数量足够多，即使是奖励都为正的动作，我们也可以按照奖励值的大小决定每个动作的优劣，并以此修改下个动作被选中的概率（好的动作增加被选中的概率，差的动作降低被选中的概率），但是在训练的环境下，总会有一些奖励为正的动作无法被采样，但是其他动作的奖励都为正，它们被选中的概率增加好，这就导致未被采样的动作的概率降低，哪怕未被采样的动作实质上优于一部分甚至全部被采样的动作。因此可以将式(7)进行如下修改：<br>$$<br>\\nabla \\overline R_\\theta &#x3D; \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}(R(\\tau^n)-b)\\nabla logp_\\theta(a_t^n|s_t^n)\\tag{8}<br>$$<br>在这里的b就是新增的baseline，通常$b \\approx E[R(\\tau)]$，这样就可以有效的降低未被采样的动作被“误杀”。</p>\n<h3 id=\"Tip-2-Assign-Suitable-Credit\"><a href=\"#Tip-2-Assign-Suitable-Credit\" class=\"headerlink\" title=\"Tip 2: Assign Suitable Credit\"></a>Tip 2: Assign Suitable Credit</h3><p>在一条Trajectory中，每个动作如果只由总的R来反映权重是不合适的，比如在$(s_a,a_1),(s_b,a_2),(s_c,a_3)$中的单步奖励值分别为+5，+0，-2，R&#x3D;5-2&#x3D;+3 但是对于a2，a3这种并未对Reward结果最大化做出正向贡献的action反而也被赋予了值为+3的Reward作为权重，这是不合理的。我们应该让每一个action前的R值都正确的反映它在当前Trajectory中的作用，到底是好还是坏。我们可以把式(8)中的$R(\\tau^n)$改写为$\\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$:<br>$$<br>\\nabla \\overline R_\\theta \\approx \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}(\\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}-b)\\nabla p_\\theta(a_t^n|s_t^n)\\tag{9}<br>$$<br>$\\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$又可以进一步写为$\\sum_{t’&#x3D;t}^{T_n}\\gamma^{t’-t} r_{t’}^{n}$，其中$\\gamma$作为discount factor(折扣因子)并且$\\gamma &lt; 1$，因此可以得到：<br>$$<br>\\nabla \\overline R_\\theta \\approx \\frac{1}{N}\\sum_{n&#x3D;1}^{N}\\sum_{t&#x3D;1}^{T_n}(\\sum_{t’&#x3D;t}^{T_n}\\gamma^{t’-t} r_{t’}^{n}-b)\\nabla p_\\theta(a_t^n|s_t^n)\\tag{10}<br>$$</p>\n<h2 id=\"On-policy-v-s-Off-policy\"><a href=\"#On-policy-v-s-Off-policy\" class=\"headerlink\" title=\"On-policy v.s. Off-policy\"></a>On-policy v.s. Off-policy</h2><ul>\n<li>On-policy：agent学习与交互的环境是相同的。</li>\n<li>Off-policy：agent学习的环境和交互的环境并不相同。</li>\n</ul>\n<h3 id=\"从On-policy转向Off-policy的分析\"><a href=\"#从On-policy转向Off-policy的分析\" class=\"headerlink\" title=\"从On- policy转向Off-policy的分析\"></a>从On- policy转向Off-policy的分析</h3><p>On-policy的情况下，Policy Gradient的公式为:<br>$$<br>\\nabla \\overline R_\\theta &#x3D; E_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla logp_\\theta(\\tau)]\\tag{11}<br>$$<br>在On-policy中，我们使用$\\pi_{\\theta}$去收集数据，当$\\theta$被更新的时候，我们必须去重新采样训练数据。</p>\n<p>我们现在的目标是：使用$\\pi_{\\theta’}$采样获得数据去训练$\\theta$。$\\theta’$是一个固定的值，因此我们可以重复利用采样的数据。</p>\n<h3 id=\"重要性采样\"><a href=\"#重要性采样\" class=\"headerlink\" title=\"重要性采样\"></a>重要性采样</h3><p>如果正常从p中进行采样获得x的期望值：<br>$$<br>E_{x\\sim p}[f(x)] \\approx \\frac{1}{N}\\sum_{i&#x3D;1}^{N}f(x^i)\\tag{12}<br>$$<br>但是现在我们不从p中采样，只能从q中采样呢？<br>$$<br>E_{x\\sim p}[f(x)] &amp;\\approx \\frac{1}{N}\\sum_{i&#x3D;1}^{N}f(x^i)<br>\\&amp;&#x3D;\\int f(x)p(x)dx \\&amp;&#x3D; \\int f(x) \\frac{p(x)}{q(x)}q(x)dx \\&amp;&#x3D; E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\\tag{13}<br>$$<br>通过这样的变换，我们就达到了从q中采样获取x期望值的效果。</p>\n<p>重要性采样公式为：<br>$$<br>E_{x\\sim p}[f(x)]&#x3D; E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\\tag{14}<br>$$</p>\n<h3 id=\"On-policy-gt-Off-policy\"><a href=\"#On-policy-gt-Off-policy\" class=\"headerlink\" title=\"On-policy-&gt;Off-policy\"></a>On-policy-&gt;Off-policy</h3><p>$$<br>\\nabla \\overline R_\\theta &amp;&#x3D; E_{(s_t,a_t)\\sim \\pi_\\theta}[A^\\theta(s_t,a_t)\\nabla logp_\\theta(a_t^n|s_t^n)]<br>\\ &amp;&#x3D;E_{(s_t,a_t)\\sim \\pi_\\theta’}[\\frac{P_\\theta(s_t,a_t)}{P_\\theta’(s_t,a_t)}A^{\\theta’}(s_t,a_t)\\nabla logp_\\theta(a_t^n|s_t^n)]<br>\\ &amp;&#x3D;E_{(s_t,a_t)\\sim \\pi_\\theta’}[\\frac{p_\\theta(a_t|s_t)}{p_\\theta’(a_t|s_t)}\\frac{p_\\theta(s_t)}{p_\\theta’(s_t)}A^{\\theta’}(s_t,a_t)\\nabla logp_\\theta(a_t^n|s_t^n)]\\tag{15}<br>$$</p>\n<p>由于我们假设$\\theta$与$\\theta’$是一样的，因此${p_\\theta(s_t)}$和 $ p_\\theta’(s_t)$这两个同环境相关的参数可以约掉，而${p_\\theta(a_t|s_t)}$和${p_\\theta’(a_t|s_t)}$是同动作选择相关，并没有假设它们的动作选择一致，因此不能约掉。因此我们得到：<br>$$<br>J^{\\theta’}(\\theta) &#x3D; E_{(s_t,a_t)\\sim \\pi_{\\theta’}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta’}(a_t|s_t)}A^{\\theta’}(s_t,a_t)]\\tag{16}<br>$$</p>\n<h3 id=\"Add-Constraint\"><a href=\"#Add-Constraint\" class=\"headerlink\" title=\"Add Constraint\"></a>Add Constraint</h3><p>$\\theta$和$\\theta’$的区别是一个值得讨论的问题。在这里我们所说的区别并不是$\\theta$和$\\theta’$参数上的不同，而是说它们在表现上的不同的程度需要被限制，而想要这个区别被限制，就必须要使它可以被量化。因此，在PPO中，使用$KL(\\theta,\\theta’)$对$\\theta$和$\\theta’$在表现上的不同的程度进行量化。由此可以得到两个算法，即Proximal Policy Optimization(PPO)和TRPO(Trust Region Policy Optimization)：</p>\n<ul>\n<li>Proximal Policy Optimization(PPO)</li>\n</ul>\n<p>$$<br>J_{PPO}^{\\theta’}(\\theta) &#x3D; J^{\\theta’}(\\theta) - \\beta KL(\\theta,\\theta’)\\tag{17}<br>$$</p>\n<ul>\n<li>TRPO(Trust Region Policy Optimization)</li>\n</ul>\n<p>$$<br>J_{TRPO}^{\\theta’}(\\theta) &#x3D; E_{(s_t,a_t)\\sim \\pi_{\\theta’}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta’}(a_t|s_t)}A^{\\theta’}(s_t,a_t)]\\tag{18}<br>$$</p>\n<p>在TRPO中，$KL(\\theta,\\theta’)$以单独的限制存在，一般为$KL(\\theta,\\theta’)&lt;\\delta$.</p>\n<p>因此，PPO算法的伪代码如下：</p>\n<ul>\n<li><p>Initial policy parameters $\\theta^0$</p>\n</li>\n<li><p>In each iteration:</p>\n<ul>\n<li>Using $\\theta^k$ to interact with the environment to collect ${s_t,a_t}$ and compute advatage $A_{\\theta^k}(st,at)$</li>\n<li>Find $\\theta$ optimizing $J_{PPO}(\\theta)$</li>\n<li>$J_{PPO}^{\\theta^k} &#x3D; J^{\\theta^k}(\\theta) - \\beta \\times KL(\\theta,\\theta^k)$ # Update parameters several times</li>\n</ul>\n</li>\n<li><p>与此同时，动态调整$\\beta$:</p>\n<ul>\n<li>If $KL(\\theta,\\theta^k) &gt; KL_{max}$,increase $\\beta$</li>\n<li>If $KL(\\theta,\\theta^k) &lt; KL_{min}$,decrease $\\beta$</li>\n</ul>\n</li>\n</ul>\n<p>这就完成了 KL Penalty的建立。</p>\n<h2 id=\"第二种PPO算法\"><a href=\"#第二种PPO算法\" class=\"headerlink\" title=\"第二种PPO算法\"></a>第二种PPO算法</h2><p>在这个PPO算法中，并不是使用KL函数，而是使用clip函数，所谓的clip函数是指$clip(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)},1-\\epsilon,1+\\epsilon)$，当$\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)}$的值小于$1-\\epsilon$时，它的值取$1-\\epsilon$,当$\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)}$的值大于$1+\\epsilon$时，它的值取$1+\\epsilon$.因此可得：<br>$$<br>J_{PPO2}^{\\theta^k}(\\theta) \\approx \\sum_{(s_t,a_t)}min(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t|s_t)}A^{\\theta^k}(s_t,a_t),clip(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)},1-\\epsilon,1+\\epsilon)A^{\\theta^k}(s_t,a_t))<br>$$<br>$\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t|s_t)}$和$clip(\\frac{p_\\theta(a_t|s_t)}{p_\\theta^k(a_t,s_t)},1-\\epsilon,1+\\epsilon)$的图像如图所示：</p>\n<p><img src=\"http://cdn.leafii.top/img/secondppo.png\" alt=\"secondppo\"></p>\n<h2 id=\"PPO的网络结构\"><a href=\"#PPO的网络结构\" class=\"headerlink\" title=\"PPO的网络结构\"></a>PPO的网络结构</h2><p>未完待续。。。</p>\n<h2 id=\"为什么说TRPO和PPO是On-policy的？\"><a href=\"#为什么说TRPO和PPO是On-policy的？\" class=\"headerlink\" title=\"为什么说TRPO和PPO是On-policy的？\"></a>为什么说TRPO和PPO是On-policy的？</h2><p><strong>首先我们明确什么是on-policy，什么是off-policy？</strong></p>\n<ul>\n<li>on-policy：就是要训练的agent跟环境互动的agent是同一个agent,也就是我们采样的网络和要优化的网络是否是同一个网络。</li>\n<li>off-policy：那肯定就是跟上面相反的。</li>\n</ul>\n<p>那么进入正题，我们一般认为PPO是off-policy的原因就是因为PPO使用actor网络去sampler然后填充经验池，然后使用这个经验池中的数据去更新这个actor多个epoch，当更新到第二个epoch的时候那么actor就变成了actor1,然而经验池中的数据仍然是actor网络采样得到的，那么就造成了从更新第二个epoch开始采样的actor和要优化的actor不是同一个网络，那么可能就会认为它是off-pocliy的。</p>\n<p>其实可以很简单的解释这个问题，根据off-policy的定义，采样的网络和要优化的网络不是一个网络，那么对于PPO来说，<strong>使用一批数据从更新actor的第二个epoch开始，数据虽然都是旧的actor采样得到的，但是我们并没有直接使用这批数据去更新我们的新的actor，而是使用imporance sampling先将数据分布不同导致的误差进行了修正</strong>。那么这个importance sampling的目的就是让这两者数据分布之间的差异尽可能的缩小，<strong>那么就可以近似理解成做了importance sampling之后的数据就是我们的更新（这里的更新指的是多个epoch更新的中间过程）后的actor采样得来的，这样就可以理解成我们要优化得actor和采样得actor是同一个actor，那么他就是on-policy的。</strong></p>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ol>\n<li><p>【DRL-16】Proximal Policy Optimization : <a href=\"https://zhuanlan.zhihu.com/p/142312072\">https://zhuanlan.zhihu.com/p/142312072</a></p>\n</li>\n<li><p>为什么说TRPO和PPO是on-policy的？:<a href=\"https://zhuanlan.zhihu.com/p/387193698\">https://zhuanlan.zhihu.com/p/387193698</a></p>\n</li>\n</ol>","categories":[],"tags":[{"name":"python","path":"api/tags/python.json"},{"name":"强化学习","path":"api/tags/强化学习.json"}]}