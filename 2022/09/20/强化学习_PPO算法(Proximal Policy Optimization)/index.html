<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Leafii"><meta name="copyright" content="Leafii"><meta name="generator" content="Hexo 6.2.0"><meta name="theme" content="hexo-theme-yun"><title>强化学习_PPO算法(Proximal Policy Optimization) | LeafiiのBlog</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.3.3/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://fastly.jsdelivr.net/npm/@unocss/runtime/mini.global.js"></script><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/png" href="../../../../favicon.ico"><link rel="mask-icon" href="../../../../favicon.ico" color="#0078E7"><link rel="preload" href="../../../../css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="../../../../js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"leafii.top","root":"/","title":["Leafii","の","博","客"],"version":"1.9.3","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"local_search":{"path":"/search.xml"},"fireworks":{"colors":null},"vendors":{"darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="../../../../css/hexo-theme-yun.css"><script src="../../../../js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="../../../../atom.xml" title="LeafiiのBlog" type="application/atom+xml"><meta name="description" content="本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习_PPO算法(Proximal Policy Optimization)">
<meta property="og:url" content="2022/09/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0_PPO%E7%AE%97%E6%B3%95(Proximal%20Policy%20Optimization)/index.html">
<meta property="og:site_name" content="LeafiiのBlog">
<meta property="og:description" content="本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://cdn.leafii.top/img/secondppo.png">
<meta property="og:image" content="http://cdn.leafii.top/img/v2-1c0bdce5fe4df17df7c4a987c4f7c9b1_1440w.jpeg">
<meta property="og:image" content="http://cdn.leafii.top/img/v2-053ff8d2e893a3ce59c67dac0406eeb6_1440w.jpeg">
<meta property="article:published_time" content="2022-09-20T13:15:18.000Z">
<meta property="article:modified_time" content="2022-10-30T07:38:33.058Z">
<meta property="article:author" content="Leafii">
<meta property="article:tag" content="python">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://cdn.leafii.top/img/secondppo.png"><script>(function() {
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="../../../../js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="../../../../js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="../../../../about/" title="Leafii"><img width="96" loading="lazy" src="../../../../images/avatar.jpg" alt="Leafii"></a><div class="site-author-name"><a href="../../../../about/">Leafii</a></div><a class="site-name" href="../../../../about/site.html">LeafiiのBlog</a><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="../../../../index.html" title="我的主页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="../../../../archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">190</span></a></div><div class="site-state-item"><a href="../../../../categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="../../../../tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">34</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="主题文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/mikutown" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:yunsenye@gmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=299583310" title="网易云音乐" target="_blank" style="color:#C10D0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="../../../../links/" title="友情链接" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RL%E7%9B%B8%E5%85%B3%E8%A6%81%E7%B4%A0"><span class="toc-number">1.</span> <span class="toc-text">RL相关要素</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Gradient"><span class="toc-number">2.</span> <span class="toc-text">Policy Gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tip-1-Add-a-Baseline"><span class="toc-number">2.1.</span> <span class="toc-text">Tip 1: Add a Baseline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tip-2-Assign-Suitable-Credit"><span class="toc-number">2.2.</span> <span class="toc-text">Tip 2: Assign Suitable Credit</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#On-policy-v-s-Off-policy"><span class="toc-number">3.</span> <span class="toc-text">On-policy v.s. Off-policy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8EOn-policy%E8%BD%AC%E5%90%91Off-policy%E7%9A%84%E5%88%86%E6%9E%90"><span class="toc-number">3.1.</span> <span class="toc-text">从On- policy转向Off-policy的分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="toc-number">3.2.</span> <span class="toc-text">重要性采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-policy-gt-Off-policy"><span class="toc-number">3.3.</span> <span class="toc-text">On-policy-&gt;Off-policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-Constraint"><span class="toc-number">3.4.</span> <span class="toc-text">Add Constraint</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%A7%8DPPO%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">第二种PPO算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">5.</span> <span class="toc-text">PPO的网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">5.1.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A7%E7%94%9Fexperience%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">5.2.</span> <span class="toc-text">产生experience的过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Actor%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9B%B4%E6%96%B0%E6%B5%81%E7%A8%8B"><span class="toc-number">5.3.</span> <span class="toc-text">Actor网络的更新流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Critic%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9B%B4%E6%96%B0%E6%B5%81%E7%A8%8B"><span class="toc-number">5.4.</span> <span class="toc-text">Critic网络的更新流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4TRPO%E5%92%8CPPO%E6%98%AFOn-policy%E7%9A%84%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">为什么说TRPO和PPO是On-policy的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion-3"><span class="toc-number">7.</span> <span class="toc-text">Conclusion[3]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">8.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https:/leafii.top"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Leafii"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="LeafiiのBlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">强化学习_PPO算法(Proximal Policy Optimization)</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2022-09-20 21:15:18" itemprop="dateCreated datePublished" datetime="2022-09-20T21:15:18+08:00">2022-09-20</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="修改时间：2022-10-30 15:38:33" itemprop="dateModified" datetime="2022-10-30T15:38:33+08:00">2022-10-30</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">4.2k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">18m</span></span></span><div class="post-classify"><span class="post-tag"><a class="tag-item" href="../../../../tags/python/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">python</span></a><a class="tag-item" href="../../../../tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">强化学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。</p>
<span id="more"></span>

<p>PPO(Proximal Policy Optimization)是OpenAI使用的默认RL方法，PPO方法可以被理解为</p>
<p><code>Policy Gradient -&gt; (On Policy -&gt; Off Policy) -&gt; (Add Constraint) -&gt; PPO(Proximal Policy Optimization)</code></p>
<h2 id="RL相关要素"><a href="#RL相关要素" class="headerlink" title="RL相关要素"></a>RL相关要素</h2><p>强化学习是指智能体在给定环境中进行动作的选择，在动作选择并执行之后同环境交互获得新的状态，每一对State（状态）和Action（动作）可以得到相应的Reward（奖励），强化学习的目标就是最大化Reward。</p>
<p>状态，动作更替可以用Trajectory（迹）来表示：</p>
<p>$$Trajectory\ \tau &#x3D; {s_1,a_1,s_2,a_2,…,s_T,a_T}\tag{1}$$</p>
<p>每一条Trajectory的概率为:</p>
<p>$$p_\theta(\tau) &#x3D;p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_2|s_2)p(s_3|s_2,a_2)…&#x3D;p(s_1)\prod_{t&#x3D;1}^{T}p_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\tag{2}$$</p>
<p>在这个式子中我们可以看到，$p_\theta(a_t|s_t)$是Actor得出的，这个是我们可以控制的，但是$p(s_{t+1}|s_t,a_t)$是动作$a_t$在状态$s_t$下与环境交互转移到状态$s_{t+1}$的概率，这是由环境本身决定的，我们自己无法控制它。</p>
<p>在强化学习中一条Trajectory的Expected Reward为：</p>
<p>$$\overline R_\theta &#x3D; \sum_{\tau}R(\tau)p_\theta(\tau)&#x3D;E_{\tau\sim p_\theta(\tau)}[R(\tau)]\tag{3})$$</p>
<p>其中的$R(\tau)$为：</p>
<p>$$R(\tau) &#x3D; \sum_{t&#x3D;1}^{T} r_t\tag{4}$$</p>
<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><p>想要最大化Reward，在上一节我们又获得了$\overline R_\theta$的公式，此时我们对$\overline R_\theta$求梯度:</p>
<p>$$\nabla \overline R_\theta &#x3D; \sum_{\tau}R(\tau)\nabla p_\theta(\tau)&#x3D;\sum_{\tau}R(\tau)p_\theta(\tau)\frac{\nabla p_\theta(\tau)}{p_\theta(\tau)}\tag{5}$$</p>
<p>在上式中，$R(\tau)$不一定是需要可微的，它甚至可以是一个黑盒。</p>
<p>$$\nabla f(x) &#x3D; f(x)\nabla logf(x)\tag{6}$$</p>
<p>接着将梯度公式(6)代入(5)中，得：</p>
<p>$$\nabla \overline R_\theta &#x3D; \sum_{\tau}R(\tau)p_\theta(\tau)\nabla logp_\theta(\tau)&#x3D; E_{\tau \sim p_\theta(\tau)}[R(\tau)\nabla logp_\theta(\tau)] \approx \frac{1}{N}\sum_{n&#x3D;1}^{N}R(\tau^n)\nabla logp_\theta(\tau^n)$$</p>
<p>$$&#x3D; \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}R(\tau^n)\nabla logp_\theta(a_t^n|s_t^n)\tag{7}$$</p>
<p>在式子(7)中，由于$\sum_{\tau}$和$p_{\theta}(\tau)$的存在，因此将它们写成期望$E_{\tau \sim p_\theta(\tau)}$的形式。$p_{\theta}(\tau)$相当于$\nabla logp_\theta(\tau)$的一个weight（权重）。并且在$p_\theta(\tau)$中，$\tau$相当于有两项，一项是来自环境本身（无法求梯度），另一项来自智能体Agent，因此将$\nabla logp_\theta(\tau)$更进一步写作$logp_\theta(a_t^n|s_t^n)$</p>
<p>因此的Policy Gradient的基本过程可以这样描述：在环境中取得数据，在给定的策略$\pi_\theta$下，获得不同的Trajectory，每个Trajectory拥有不同的状态，动作以及对应的奖励值，收集状态动作对之后，将其带入式(7)中，进行参数$\theta$的更新：$\theta \leftarrow \theta + \eta\nabla \overline R_\theta$,在更新之后对新的环境重新获取数据，循环往复，我们可以看到在这个流程中，在环境中取得的数据仅被使用了一次，因此Policy Gradient是一个严格的On-Policy算法。</p>
<h3 id="Tip-1-Add-a-Baseline"><a href="#Tip-1-Add-a-Baseline" class="headerlink" title="Tip 1: Add a Baseline"></a>Tip 1: Add a Baseline</h3><p>在某些情况下，$\theta \leftarrow \theta + \eta\nabla \overline R_\theta$中的$R(\tau^n)$始终为正，如果采样数量足够多，即使是奖励都为正的动作，我们也可以按照奖励值的大小决定每个动作的优劣，并以此修改下个动作被选中的概率（好的动作增加被选中的概率，差的动作降低被选中的概率），但是在训练的环境下，总会有一些奖励为正的动作无法被采样，但是其他动作的奖励都为正，它们被选中的概率增加好，这就导致未被采样的动作的概率降低，哪怕未被采样的动作实质上优于一部分甚至全部被采样的动作。因此可以将式(7)进行如下修改：</p>
<p>$$\nabla \overline R_\theta &#x3D; \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}(R(\tau^n)-b)\nabla logp_\theta(a_t^n|s_t^n)\tag{8}$$</p>
<p>在这里的b就是新增的baseline，通常$b \approx E[R(\tau)]$，这样就可以有效的降低未被采样的动作被“误杀”。</p>
<h3 id="Tip-2-Assign-Suitable-Credit"><a href="#Tip-2-Assign-Suitable-Credit" class="headerlink" title="Tip 2: Assign Suitable Credit"></a>Tip 2: Assign Suitable Credit</h3><p>在一条Trajectory中，每个动作如果只由总的R来反映权重是不合适的，比如在$(s_a,a_1),(s_b,a_2),(s_c,a_3)$中的单步奖励值分别为+5，+0，-2，R&#x3D;5-2&#x3D;+3 但是对于a2，a3这种并未对Reward结果最大化做出正向贡献的action反而也被赋予了值为+3的Reward作为权重，这是不合理的。我们应该让每一个action前的R值都正确的反映它在当前Trajectory中的作用，到底是好还是坏。我们可以把式(8)中的$R(\tau^n)$改写为$\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$:</p>
<p>$$\nabla \overline R_\theta \approx \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}(\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}-b)\nabla p_\theta(a_t^n|s_t^n)\tag{9}$$</p>
<p>$\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$又可以进一步写为$\sum_{t’&#x3D;t}^{T_n}\gamma^{t’-t} r_{t’}^{n}$，其中$\gamma$作为discount factor(折扣因子)并且$\gamma &lt; 1$，因此可以得到：</p>
<p>$$\nabla \overline R_\theta \approx \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}(\sum_{t’&#x3D;t}^{T_n}\gamma^{t’-t} r_{t’}^{n}-b)\nabla p_\theta(a_t^n|s_t^n)\tag{10}$$</p>
<h2 id="On-policy-v-s-Off-policy"><a href="#On-policy-v-s-Off-policy" class="headerlink" title="On-policy v.s. Off-policy"></a>On-policy v.s. Off-policy</h2><ul>
<li>On-policy：agent学习与交互的环境是相同的。<strong>On-Policy可以翻译为”同策略”</strong></li>
<li>Off-policy：agent学习的环境和交互的环境并不相同。<strong>Off-Policy可以翻译为”异策略”</strong></li>
</ul>
<h3 id="从On-policy转向Off-policy的分析"><a href="#从On-policy转向Off-policy的分析" class="headerlink" title="从On- policy转向Off-policy的分析"></a>从On- policy转向Off-policy的分析</h3><p>On-policy的情况下，Policy Gradient的公式为:</p>
<p>$$\nabla \overline R_\theta &#x3D; E_{\tau \sim p_\theta(\tau)}[R(\tau)\nabla logp_\theta(\tau)]\tag{11}$$</p>
<p>在On-policy中，我们使用$\pi_{\theta}$去收集数据，当$\theta$被更新的时候，我们必须去重新采样训练数据。</p>
<p>我们现在的目标是：使用$\pi_{\theta’}$采样获得数据去训练$\theta$。$\theta’$是一个固定的值，因此我们可以重复利用采样的数据。</p>
<h3 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h3><p>如果正常从p中进行采样获得x的期望值：</p>
<p>$$E_{x\sim p}[f(x)] \approx \frac{1}{N}\sum_{i&#x3D;1}^{N}f(x^i)\tag{12}$$</p>
<p>但是现在我们不从p中采样，只能从q中采样呢？</p>
<p>$$E_{x\sim p}[f(x)] \approx \frac{1}{N}\sum_{i&#x3D;1}^{N}f(x^i)&#x3D;\int f(x)p(x)dx&#x3D; \int f(x) \frac{p(x)}{q(x)}q(x)dx&#x3D; E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\tag{13}$$</p>
<p>通过这样的变换，我们就达到了从q中采样获取x期望值的效果。</p>
<p>重要性采样公式为：</p>
<p>$$E_{x\sim p}[f(x)]&#x3D; E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\tag{14}$$</p>
<h3 id="On-policy-gt-Off-policy"><a href="#On-policy-gt-Off-policy" class="headerlink" title="On-policy-&gt;Off-policy"></a>On-policy-&gt;Off-policy</h3><p>$$\nabla \overline R_\theta &#x3D; E_{(s_t,a_t)\sim \pi_\theta}[A^\theta(s_t,a_t)\nabla logp_\theta(a_t^n|s_t^n)]&#x3D;E_{(s_t,a_t)\sim \pi_\theta’}[\frac{P_\theta(s_t,a_t)}{P_\theta’(s_t,a_t)}A^{\theta’}(s_t,a_t)\nabla logp_\theta(a_t^n|s_t^n)]$$</p>
<p>$$&#x3D;E_{(s_t,a_t)\sim \pi_\theta’}[\frac{p_\theta(a_t|s_t)}{p_\theta’(a_t|s_t)}\frac{p_\theta(s_t)}{p_\theta’(s_t)}A^{\theta’}(s_t,a_t)\nabla logp_\theta(a_t^n|s_t^n)]\tag{15}$$</p>
<p>由于我们假设$\theta$与$\theta’$是一样的，因此${p_\theta(s_t)}$和 $ p_\theta’(s_t)$这两个同环境相关的参数可以约掉，而${p_\theta(a_t|s_t)}$和${p_\theta’(a_t|s_t)}$是同动作选择相关，并没有假设它们的动作选择一致，因此不能约掉。因此我们得到：</p>
<p>$$J^{\theta’}(\theta) &#x3D; E_{(s_t,a_t)\sim \pi_{\theta’}}[\frac{p_\theta(a_t|s_t)}{p_{\theta’}(a_t|s_t)}A^{\theta’}(s_t,a_t)]\tag{16}$$</p>
<h3 id="Add-Constraint"><a href="#Add-Constraint" class="headerlink" title="Add Constraint"></a>Add Constraint</h3><p>$\theta$和$\theta’$的区别是一个值得讨论的问题。在这里我们所说的区别并不是$\theta$和$\theta’$参数上的不同，而是说它们在表现上的不同的程度需要被限制，而想要这个区别被限制，就必须要使它可以被量化。因此，在PPO中，使用$KL(\theta,\theta’)$对$\theta$和$\theta’$在表现上的不同的程度进行量化。由此可以得到两个算法，即Proximal Policy Optimization(PPO)和TRPO(Trust Region Policy Optimization)：</p>
<ul>
<li>Proximal Policy Optimization(PPO)</li>
</ul>
<p>$$J_{PPO}^{\theta’}(\theta) &#x3D; J^{\theta’}(\theta) - \beta KL(\theta,\theta’)\tag{17}$$</p>
<ul>
<li>TRPO(Trust Region Policy Optimization)</li>
</ul>
<p>$$J_{TRPO}^{\theta’}(\theta) &#x3D; E_{(s_t,a_t)\sim \pi_{\theta’}}[\frac{p_\theta(a_t|s_t)}{p_{\theta’}(a_t|s_t)}A^{\theta’}(s_t,a_t)]\tag{18}$$</p>
<p>在TRPO中，$KL(\theta,\theta’)$以单独的限制存在，一般为$KL(\theta,\theta’)&lt;\delta$.</p>
<p>因此，PPO算法的伪代码如下：</p>
<ul>
<li><p>Initial policy parameters $\theta^0$</p>
</li>
<li><p>In each iteration:</p>
<ul>
<li>Using $\theta^k$ to interact with the environment to collect ${s_t,a_t}$ and compute advatage $A_{\theta^k}(st,at)$</li>
<li>Find $\theta$ optimizing $J_{PPO}(\theta)$</li>
<li>$J_{PPO}^{\theta^k} &#x3D; J^{\theta^k}(\theta) - \beta \times KL(\theta,\theta^k)$ # Update parameters several times</li>
</ul>
</li>
<li><p>与此同时，动态调整$\beta$:</p>
<ul>
<li>If $KL(\theta,\theta^k) &gt; KL_{max}$,increase $\beta$</li>
<li>If $KL(\theta,\theta^k) &lt; KL_{min}$,decrease $\beta$</li>
</ul>
</li>
</ul>
<p>这就完成了 KL Penalty的建立。</p>
<h2 id="第二种PPO算法"><a href="#第二种PPO算法" class="headerlink" title="第二种PPO算法"></a>第二种PPO算法</h2><p>在这个PPO算法中，并不是使用KL函数，而是使用clip函数，所谓的clip函数是指$clip(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)},1-\epsilon,1+\epsilon)$，当$\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)}$的值小于$1-\epsilon$时，它的值取$1-\epsilon$,当$\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)}$的值大于$1+\epsilon$时，它的值取$1+\epsilon$.因此可得：</p>
<p>$$J_{PPO2}^{\theta^k}(\theta) \approx \sum_{(s_t,a_t)}min(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t|s_t)}A^{\theta^k}(s_t,a_t),clip(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)},1-\epsilon,1+\epsilon)A^{\theta^k}(s_t,a_t))\tag{19}$$</p>
<p>$\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t|s_t)}$和$clip(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)},1-\epsilon,1+\epsilon)$的图像如图所示：</p>
<p><img src="http://cdn.leafii.top/img/secondppo.png" alt="secondppo" loading="lazy"></p>
<h2 id="PPO的网络结构"><a href="#PPO的网络结构" class="headerlink" title="PPO的网络结构"></a>PPO的网络结构</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>一个actor网络，一个critic网络</p>
<p><img src="http://cdn.leafii.top/img/v2-1c0bdce5fe4df17df7c4a987c4f7c9b1_1440w.jpeg" alt="img" loading="lazy"></p>
<ul>
<li>actor网络的输入为状态，输出为动作概率$\pi(a_t|s_t)$(对于离散动作空间而言)或者动作概率分布参数（对于连续动作空间而言）</li>
<li>critic网络的输入为状态，输出为状态的价值。</li>
</ul>
<p>显然，如果actor网络输出的动作能够使优势（$A^\theta(s_t,a_t)$）变大，那么就越好。如果critic网络输出的状态价值越准确，那么就越好。</p>
<h3 id="产生experience的过程"><a href="#产生experience的过程" class="headerlink" title="产生experience的过程"></a>产生experience的过程</h3><p>已知一个状态$s_0$，通过actor网络得到所有动作的概率（图中以三个动作：$a,b,c$为例），然后依概率采样得到动作$a_0$，然后将$a_0$输入到环境中，得到$s_1$和$r_1$。状态价值$v(s_0)$是通过critic网络输出得到的，这样就得到一个experience：$(s_0,a_0,r_1,v(s_0),logP(a_0|s_0))$，然后将experience放入经验池中(当然之后还会计算$A(s_0,a_0))$以及$G_0$，经验池中也存放了这两个信息)。</p>
<p><img src="http://cdn.leafii.top/img/v2-053ff8d2e893a3ce59c67dac0406eeb6_1440w.jpeg" alt="img" loading="lazy"></p>
<blockquote>
<p>注：虽然$v(s_0)$可以用一条轨迹的折扣回报得到，即：$v(s_0)&#x3D;r_1+\gamma r_2+…+\gamma^{T}r_{T+1}+\gamma^{T+1}v(s_{T+1})$,但是轨迹末状态的下一状态$s_{T+1}$的$v(s_{T+1})$还是需要critic网络来估计，当然如果$s_{T+1}$是正常游戏结束，而不是达到了最大步长，那么令$v(s_{T+1}&#x3D;0$)。与其这样，还不如直接用critic网络直接估计$v(s_0)$，而且值得注意的是，$v(s_0)&#x3D;r_1+\gamma r_2+…+\gamma^{T}r_{T+1}+\gamma^{T+1}v(s_{T+1})$正是我们critic网络作为监督学习的真值</p>
</blockquote>
<p>以上是离散动作的情况，如果是连续动作，就输出概率分布的参数（比如高斯分布的均值和方差），然后按照概率分布去采样得到动作$a_0$。</p>
<p><strong>经验池</strong>存在的意义是更加方便的计算一条轨迹上状态的累积折扣回报$v(s_t)$以及优势$A(s_t,a_t)$而不是消除experience的相关性。</p>
<h3 id="Actor网络的更新流程"><a href="#Actor网络的更新流程" class="headerlink" title="Actor网络的更新流程"></a>Actor网络的更新流程</h3><p>对优势函数进行定义：</p>
<p>$$\hat A_t &#x3D; \delta_t+(\gamma \lambda)\delta_{t+1}+…+…+(\gamma \lambda)^{T-t+1}\delta_{T-1},\tag{20}$$</p>
<p>$$where \delta_t &#x3D; r_t + \gamma V(s_{t+1}) - V(s_t)\tag{21}$$</p>
<p>因为Actor网络需要输出的动作优势尽可能的大，所以它的训练需要用以下表达式作为Loss函数:</p>
<p>$$L^{CLIP}(\theta) &#x3D; \hat {\mathbb E}_t[min(r_t(\theta)\hat A_t,clip(r_t(\theta),1-\epsilon,1+\epsilon)\hat A_t\tag{22}$$</p>
<p>其中$$r_t(\theta) &#x3D; \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$</p>
<p><strong>值得注意的是：</strong> 和TD3算法的单步TD不同，PPO算法使用多步TD，因此它需要跑完一条轨迹后，才开始计算各个<strong>状态的累积回报</strong>和<strong>动作的优势</strong>。具体而言，状态价值 ，$v(s_0)$，$v(s_1)$ 是通过critic网络输出得到的，动作优势 $A(s_0,a_0)$ 是通过首先计算$ \delta_0&#x3D;r_1+v(s_1)−v(s_0) $，然后用 $\gamma \lambda$作为折扣因子去计算动作优势 $A(s_0,a_0)$ ，具体可以看公式（20）。</p>
<p>因此训练actor网络的时候需要将经验池中的所有数据都拿出来，计算loss，然后用梯度上升法，多更新几步梯度。更新完成后即将经验池清空，等待下一个新的actor网络与环境互动去收集数据。</p>
<p>PyTorch 代码如下：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># train actor net</span>
        all_pi_tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>actor_net<span class="token punctuation">(</span>state_tensor<span class="token punctuation">)</span>
        pi_tensor <span class="token operator">=</span> all_pi_tensor<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> action_tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        surrogate_advantage_tensor <span class="token operator">=</span> <span class="token punctuation">(</span>pi_tensor <span class="token operator">/</span> old_pi_tensor<span class="token punctuation">)</span> <span class="token operator">*</span>                 advantage_tensor
        clip_times_advantage_tensor <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token operator">*</span> surrogate_advantage_tensor
        max_surrogate_advantage_tensor <span class="token operator">=</span> advantage_tensor <span class="token operator">+</span>                 torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>advantage_tensor <span class="token operator">></span> <span class="token number">0.</span><span class="token punctuation">,</span>
                clip_times_advantage_tensor<span class="token punctuation">,</span> <span class="token operator">-</span>clip_times_advantage_tensor<span class="token punctuation">)</span>
        clipped_surrogate_advantage_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>
                surrogate_advantage_tensor<span class="token punctuation">,</span> max_surrogate_advantage_tensor<span class="token punctuation">)</span>
        actor_loss_tensor <span class="token operator">=</span> <span class="token operator">-</span>clipped_surrogate_advantage_tensor<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        actor_loss_tensor<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<h3 id="Critic网络的更新流程"><a href="#Critic网络的更新流程" class="headerlink" title="Critic网络的更新流程"></a>Critic网络的更新流程</h3><p>Actor网络更新后，接着拿从经验池buffer中采出的数据进行Critic网络的更新（数据已经计算了状态价值，折扣回报$G_t$的计算是基于多步TD的方法，从那个状态开始，用每一步环境返回的奖励 R 与折扣因子相乘后累加，即：$G_t &#x3D; r_{t+1} + \gamma r_{t+2} + … + \gamma^{T-t}r_{T+1} + \gamma^{T+1-t}v(s_{T+1})$，其中$v(s_{T+1})$为网络的估计值，更新方式即为：计算好的折扣回报 $G_T$与Critic网络预测当前状态价值 $v(s_t)$ 做差，用MSEloss作为Loss函数，对神经网络进行训练。</p>
<p>pytorch代码如下：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># train critic net</span>
        pred_tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>critic_net<span class="token punctuation">(</span>state_tensor<span class="token punctuation">)</span>
        critic_loss_tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>critic_loss<span class="token punctuation">(</span>pred_tensor<span class="token punctuation">,</span> return_tensor<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        critic_loss_tensor<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<h2 id="为什么说TRPO和PPO是On-policy的？"><a href="#为什么说TRPO和PPO是On-policy的？" class="headerlink" title="为什么说TRPO和PPO是On-policy的？"></a>为什么说TRPO和PPO是On-policy的？</h2><p><strong>首先我们明确什么是on-policy，什么是off-policy？</strong></p>
<ul>
<li>on-policy：就是要训练的agent跟环境互动的agent是同一个agent,也就是我们采样的网络和要优化的网络是否是同一个网络。</li>
<li>off-policy：那肯定就是跟上面相反的。</li>
</ul>
<p>那么进入正题，我们一般认为PPO是off-policy的原因就是因为PPO使用actor网络去sampler然后填充经验池，然后使用这个经验池中的数据去更新这个actor多个epoch，当更新到第二个epoch的时候那么actor就变成了actor1,然而经验池中的数据仍然是actor网络采样得到的，那么就造成了从更新第二个epoch开始采样的actor和要优化的actor不是同一个网络，那么可能就会认为它是off-pocliy的。</p>
<p>其实可以很简单的解释这个问题，根据off-policy的定义，采样的网络和要优化的网络不是一个网络，那么对于PPO来说，<strong>使用一批数据从更新actor的第二个epoch开始，数据虽然都是旧的actor采样得到的，但是我们并没有直接使用这批数据去更新我们的新的actor，而是使用imporance sampling先将数据分布不同导致的误差进行了修正</strong>。那么这个importance sampling的目的就是让这两者数据分布之间的差异尽可能的缩小，<strong>那么就可以近似理解成做了importance sampling之后的数据就是我们的更新（这里的更新指的是多个epoch更新的中间过程）后的actor采样得来的，这样就可以理解成我们要优化得actor和采样得actor是同一个actor，那么他就是on-policy的。</strong></p>
<h2 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion[3]"></a>Conclusion[3]</h2><p>We have introduced proximal policy optimization, a family of policy optimization methods that <strong>use multiple epochs of stochastic gradient ascent to perform each policy update</strong>.These methods <strong>have the stability and reliability of trust-region methods but are much simpler to implement</strong>**, requiring only few lines of code change to a vanilla policy gradient implementation, applicable in more general settings (for example, when <strong>using a joint architecture for the policy and value function</strong>), and have better overall performance.</p>
<p>我们介绍了近程策略优化，这是一系列策略优化方法，使用随机梯度上升的多个周期来执行每个策略更新。 这些方法具有信任域方法的稳定性和可靠性，但实现起来要简单得多，只需要很少的代码行就可以改变成一个普通的策略梯度实现，适用于更一般的设置（例如，当使用策略和值函数的联合体系结构时），并且具有更好的整体性能。 </p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><p>【DRL-16】Proximal Policy Optimization : <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/142312072">https://zhuanlan.zhihu.com/p/142312072</a></p>
</li>
<li><p>为什么说TRPO和PPO是on-policy的？:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/387193698">https://zhuanlan.zhihu.com/p/387193698</a></p>
</li>
<li><p>Proximal Policy Optimization Algorithms:arXiv:1707.06347v2 [cs.LG] 28 Aug 2017</p>
</li>
</ol>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Leafii</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="2022/09/20/强化学习_PPO算法(Proximal Policy Optimization)/" title="强化学习_PPO算法(Proximal Policy Optimization)">2022/09/20/强化学习_PPO算法(Proximal Policy Optimization)/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="../../30/MAPPO%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/" rel="prev" title="MAPPO源代码分析"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">MAPPO源代码分析</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="../../../08/09/%E7%94%A8SHAP%E8%A7%A3%E9%87%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="next" title="用SHAP解释机器学习"><span class="post-nav-text">用SHAP解释机器学习</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2023 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> Leafii</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.9.3</span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="../../../../js/search/local-search.js" defer type="module"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="搜索..." value=""></div><div id="local-search-result"></div></div><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></div></body></html>