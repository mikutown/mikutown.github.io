<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Leafii"><meta name="copyright" content="Leafii"><meta name="generator" content="Hexo 6.2.0"><meta name="theme" content="hexo-theme-yun"><title>强化学习_PPO算法(Proximal Policy Optimization) | LeafiiのBlog</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.3.3/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://fastly.jsdelivr.net/npm/@unocss/runtime/mini.global.js"></script><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="icon" type="image/png" href="../../../../favicon.ico"><link rel="mask-icon" href="../../../../favicon.ico" color="#0078E7"><link rel="preload" href="../../../../css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="../../../../js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"leafii.top","root":"/","title":["Leafii","の","博","客"],"version":"1.9.3","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"local_search":{"path":"/search.xml"},"fireworks":{"colors":null},"vendors":{"darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="../../../../css/hexo-theme-yun.css"><script src="../../../../js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="../../../../atom.xml" title="LeafiiのBlog" type="application/atom+xml"><meta name="description" content="本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习_PPO算法(Proximal Policy Optimization)">
<meta property="og:url" content="2022/09/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0_PPO%E7%AE%97%E6%B3%95(Proximal%20Policy%20Optimization)/index.html">
<meta property="og:site_name" content="LeafiiのBlog">
<meta property="og:description" content="本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://cdn.leafii.top/img/secondppo.png">
<meta property="article:published_time" content="2022-09-20T13:15:18.000Z">
<meta property="article:modified_time" content="2022-09-23T11:48:34.928Z">
<meta property="article:author" content="Leafii">
<meta property="article:tag" content="python">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://cdn.leafii.top/img/secondppo.png"><script>(function() {
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="../../../../js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="../../../../js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="../../../../about/" title="Leafii"><img width="96" loading="lazy" src="../../../../images/avatar.jpg" alt="Leafii"></a><div class="site-author-name"><a href="../../../../about/">Leafii</a></div><a class="site-name" href="../../../../about/site.html">LeafiiのBlog</a><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="../../../../index.html" title="我的主页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="../../../../archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">65</span></a></div><div class="site-state-item"><a href="../../../../categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="../../../../tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">22</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="主题文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/mikutown" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:yeyunsen@outlook.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=299583310" title="网易云音乐" target="_blank" style="color:#C10D0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="../../../../links/" title="友情链接" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RL%E7%9B%B8%E5%85%B3%E8%A6%81%E7%B4%A0"><span class="toc-number">1.</span> <span class="toc-text">RL相关要素</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Gradient"><span class="toc-number">2.</span> <span class="toc-text">Policy Gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tip-1-Add-a-Baseline"><span class="toc-number">2.1.</span> <span class="toc-text">Tip 1: Add a Baseline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tip-2-Assign-Suitable-Credit"><span class="toc-number">2.2.</span> <span class="toc-text">Tip 2: Assign Suitable Credit</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#On-policy-v-s-Off-policy"><span class="toc-number">3.</span> <span class="toc-text">On-policy v.s. Off-policy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8EOn-policy%E8%BD%AC%E5%90%91Off-policy%E7%9A%84%E5%88%86%E6%9E%90"><span class="toc-number">3.1.</span> <span class="toc-text">从On- policy转向Off-policy的分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="toc-number">3.2.</span> <span class="toc-text">重要性采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-policy-gt-Off-policy"><span class="toc-number">3.3.</span> <span class="toc-text">On-policy-&gt;Off-policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-Constraint"><span class="toc-number">3.4.</span> <span class="toc-text">Add Constraint</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%A7%8DPPO%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">第二种PPO算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">5.</span> <span class="toc-text">PPO的网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4TRPO%E5%92%8CPPO%E6%98%AFOn-policy%E7%9A%84%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">为什么说TRPO和PPO是On-policy的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">7.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https:/leafii.top"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Leafii"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="LeafiiのBlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">强化学习_PPO算法(Proximal Policy Optimization)</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2022-09-20 21:15:18" itemprop="dateCreated datePublished" datetime="2022-09-20T21:15:18+08:00">2022-09-20</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="修改时间：2022-09-23 19:48:34" itemprop="dateModified" datetime="2022-09-23T19:48:34+08:00">2022-09-23</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">2.9k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">12m</span></span></span><div class="post-classify"><span class="post-tag"><a class="tag-item" href="../../../../tags/python/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">python</span></a><a class="tag-item" href="../../../../tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">强化学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>本文介绍强化学习中的PPO(Proximal Policy Optimization)算法。</p>
<span id="more"></span>

<p>PPO(Proximal Policy Optimization)是OpenAI使用的默认RL方法，PPO方法可以被理解为</p>
<p><code>Policy Gradient -&gt; (On Policy -&gt; Off Policy) -&gt; (Add Constraint) -&gt; PPO(Proximal Policy Optimization)</code></p>
<h2 id="RL相关要素"><a href="#RL相关要素" class="headerlink" title="RL相关要素"></a>RL相关要素</h2><p>强化学习是指智能体在给定环境中进行动作的选择，在动作选择并执行之后同环境交互获得新的状态，每一对State（状态）和Action（动作）可以得到相应的Reward（奖励），强化学习的目标就是最大化Reward。</p>
<p>状态，动作更替可以用Trajectory（迹）来表示：<br>$$<br>Trajectory\ \tau &#x3D; {s_1,a_1,s_2,a_2,…,s_T,a_T}\tag{1}<br>$$<br>每一条Trajectory的概率为:<br>$$<br>p_\theta(\tau) &amp;&#x3D; p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_2|s_2)p(s_3|s_2,a_2)…<br>\&amp;&#x3D;p(s_1)\prod_{t&#x3D;1}^{T}p_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\tag{2}<br>$$<br>在这个式子中我们可以看到，$p_\theta(a_t|s_t)$是Actor得出的，这个是我们可以控制的，但是$p(s_{t+1}|s_t,a_t)$是动作$a_t$在状态$s_t$下与环境交互转移到状态$s_{t+1}$的概率，这是由环境本身决定的，我们自己无法控制它。</p>
<p>在强化学习中一条Trajectory的Expected Reward为：<br>$$<br>\overline R_\theta &#x3D; \sum_{\tau}R(\tau)p_\theta(\tau)&#x3D;E_{\tau\sim p_\theta(\tau)}[R(\tau)]\tag{3})<br>$$<br>其中的$R(\tau)$为：<br>$$<br>R(\tau) &#x3D; \sum_{t&#x3D;1}^{T} r_t\tag{4}<br>$$</p>
<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><p>想要最大化Reward，在上一节我们又获得了$\overline R_\theta$的公式，此时我们对$\overline R_\theta$求梯度:<br>$$<br>\nabla \overline R_\theta &#x3D; \sum_{\tau}R(\tau)\nabla p_\theta(\tau)&#x3D;\sum_{\tau}R(\tau)p_\theta(\tau)\frac{\nabla p_\theta(\tau)}{p_\theta(\tau)}\tag{5}<br>$$<br>在上式中，$R(\tau)$不一定是需要可微的，它甚至可以是一个黑盒。<br>$$<br>\nabla f(x) &#x3D; f(x)\nabla logf(x)\tag{6}<br>$$<br>接着将梯度公式(6)代入(5)中，得：<br>$$<br>\nabla \overline R_\theta &amp;&#x3D; \sum_{\tau}R(\tau)p_\theta(\tau)\nabla logp_\theta(\tau)<br>\&amp;&#x3D; E_{\tau \sim p_\theta(\tau)}[R(\tau)\nabla logp_\theta(\tau)]<br>\&amp; \approx \frac{1}{N}\sum_{n&#x3D;1}^{N}R(\tau^n)\nabla logp_\theta(\tau^n)<br>\ &amp;&#x3D; \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}R(\tau^n)\nabla logp_\theta(a_t^n|s_t^n)\tag{7}<br>$$<br>在式子(7)中，由于$\sum_{\tau}$和$p_{\theta}(\tau)$的存在，因此将它们写成期望$E_{\tau \sim p_\theta(\tau)}$的形式。$p_{\theta}(\tau)$相当于$\nabla logp_\theta(\tau)$的一个weight（权重）。并且在$p_\theta(\tau)$中，$\tau$相当于有两项，一项是来自环境本身（无法求梯度），另一项来自智能体Agent，因此将$\nabla logp_\theta(\tau)$更进一步写作$logp_\theta(a_t^n|s_t^n)$</p>
<p>因此的Policy Gradient的基本过程可以这样描述：在环境中取得数据，在给定的策略$\pi_\theta$下，获得不同的Trajectory，每个Trajectory拥有不同的状态，动作以及对应的奖励值，收集状态动作对之后，将其带入式(7)中，进行参数$\theta$的更新：$\theta \leftarrow \theta + \eta\nabla \overline R_\theta$,在更新之后对新的环境重新获取数据，循环往复，我们可以看到在这个流程中，在环境中取得的数据仅被使用了一次，因此Policy Gradient是一个严格的On-Policy算法。</p>
<h3 id="Tip-1-Add-a-Baseline"><a href="#Tip-1-Add-a-Baseline" class="headerlink" title="Tip 1: Add a Baseline"></a>Tip 1: Add a Baseline</h3><p>在某些情况下，$\theta \leftarrow \theta + \eta\nabla \overline R_\theta$中的$R(\tau^n)$始终为正，如果采样数量足够多，即使是奖励都为正的动作，我们也可以按照奖励值的大小决定每个动作的优劣，并以此修改下个动作被选中的概率（好的动作增加被选中的概率，差的动作降低被选中的概率），但是在训练的环境下，总会有一些奖励为正的动作无法被采样，但是其他动作的奖励都为正，它们被选中的概率增加好，这就导致未被采样的动作的概率降低，哪怕未被采样的动作实质上优于一部分甚至全部被采样的动作。因此可以将式(7)进行如下修改：<br>$$<br>\nabla \overline R_\theta &#x3D; \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}(R(\tau^n)-b)\nabla logp_\theta(a_t^n|s_t^n)\tag{8}<br>$$<br>在这里的b就是新增的baseline，通常$b \approx E[R(\tau)]$，这样就可以有效的降低未被采样的动作被“误杀”。</p>
<h3 id="Tip-2-Assign-Suitable-Credit"><a href="#Tip-2-Assign-Suitable-Credit" class="headerlink" title="Tip 2: Assign Suitable Credit"></a>Tip 2: Assign Suitable Credit</h3><p>在一条Trajectory中，每个动作如果只由总的R来反映权重是不合适的，比如在$(s_a,a_1),(s_b,a_2),(s_c,a_3)$中的单步奖励值分别为+5，+0，-2，R&#x3D;5-2&#x3D;+3 但是对于a2，a3这种并未对Reward结果最大化做出正向贡献的action反而也被赋予了值为+3的Reward作为权重，这是不合理的。我们应该让每一个action前的R值都正确的反映它在当前Trajectory中的作用，到底是好还是坏。我们可以把式(8)中的$R(\tau^n)$改写为$\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$:<br>$$<br>\nabla \overline R_\theta \approx \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}(\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}-b)\nabla p_\theta(a_t^n|s_t^n)\tag{9}<br>$$<br>$\sum_{t’&#x3D;t}^{T_n}r_{t’}^{n}$又可以进一步写为$\sum_{t’&#x3D;t}^{T_n}\gamma^{t’-t} r_{t’}^{n}$，其中$\gamma$作为discount factor(折扣因子)并且$\gamma &lt; 1$，因此可以得到：<br>$$<br>\nabla \overline R_\theta \approx \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}(\sum_{t’&#x3D;t}^{T_n}\gamma^{t’-t} r_{t’}^{n}-b)\nabla p_\theta(a_t^n|s_t^n)\tag{10}<br>$$</p>
<h2 id="On-policy-v-s-Off-policy"><a href="#On-policy-v-s-Off-policy" class="headerlink" title="On-policy v.s. Off-policy"></a>On-policy v.s. Off-policy</h2><ul>
<li>On-policy：agent学习与交互的环境是相同的。</li>
<li>Off-policy：agent学习的环境和交互的环境并不相同。</li>
</ul>
<h3 id="从On-policy转向Off-policy的分析"><a href="#从On-policy转向Off-policy的分析" class="headerlink" title="从On- policy转向Off-policy的分析"></a>从On- policy转向Off-policy的分析</h3><p>On-policy的情况下，Policy Gradient的公式为:<br>$$<br>\nabla \overline R_\theta &#x3D; E_{\tau \sim p_\theta(\tau)}[R(\tau)\nabla logp_\theta(\tau)]\tag{11}<br>$$<br>在On-policy中，我们使用$\pi_{\theta}$去收集数据，当$\theta$被更新的时候，我们必须去重新采样训练数据。</p>
<p>我们现在的目标是：使用$\pi_{\theta’}$采样获得数据去训练$\theta$。$\theta’$是一个固定的值，因此我们可以重复利用采样的数据。</p>
<h3 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h3><p>如果正常从p中进行采样获得x的期望值：<br>$$<br>E_{x\sim p}[f(x)] \approx \frac{1}{N}\sum_{i&#x3D;1}^{N}f(x^i)\tag{12}<br>$$<br>但是现在我们不从p中采样，只能从q中采样呢？<br>$$<br>E_{x\sim p}[f(x)] &amp;\approx \frac{1}{N}\sum_{i&#x3D;1}^{N}f(x^i)<br>\&amp;&#x3D;\int f(x)p(x)dx \&amp;&#x3D; \int f(x) \frac{p(x)}{q(x)}q(x)dx \&amp;&#x3D; E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\tag{13}<br>$$<br>通过这样的变换，我们就达到了从q中采样获取x期望值的效果。</p>
<p>重要性采样公式为：<br>$$<br>E_{x\sim p}[f(x)]&#x3D; E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\tag{14}<br>$$</p>
<h3 id="On-policy-gt-Off-policy"><a href="#On-policy-gt-Off-policy" class="headerlink" title="On-policy-&gt;Off-policy"></a>On-policy-&gt;Off-policy</h3><p>$$<br>\nabla \overline R_\theta &amp;&#x3D; E_{(s_t,a_t)\sim \pi_\theta}[A^\theta(s_t,a_t)\nabla logp_\theta(a_t^n|s_t^n)]<br>\ &amp;&#x3D;E_{(s_t,a_t)\sim \pi_\theta’}[\frac{P_\theta(s_t,a_t)}{P_\theta’(s_t,a_t)}A^{\theta’}(s_t,a_t)\nabla logp_\theta(a_t^n|s_t^n)]<br>\ &amp;&#x3D;E_{(s_t,a_t)\sim \pi_\theta’}[\frac{p_\theta(a_t|s_t)}{p_\theta’(a_t|s_t)}\frac{p_\theta(s_t)}{p_\theta’(s_t)}A^{\theta’}(s_t,a_t)\nabla logp_\theta(a_t^n|s_t^n)]\tag{15}<br>$$</p>
<p>由于我们假设$\theta$与$\theta’$是一样的，因此${p_\theta(s_t)}$和 $ p_\theta’(s_t)$这两个同环境相关的参数可以约掉，而${p_\theta(a_t|s_t)}$和${p_\theta’(a_t|s_t)}$是同动作选择相关，并没有假设它们的动作选择一致，因此不能约掉。因此我们得到：<br>$$<br>J^{\theta’}(\theta) &#x3D; E_{(s_t,a_t)\sim \pi_{\theta’}}[\frac{p_\theta(a_t|s_t)}{p_{\theta’}(a_t|s_t)}A^{\theta’}(s_t,a_t)]\tag{16}<br>$$</p>
<h3 id="Add-Constraint"><a href="#Add-Constraint" class="headerlink" title="Add Constraint"></a>Add Constraint</h3><p>$\theta$和$\theta’$的区别是一个值得讨论的问题。在这里我们所说的区别并不是$\theta$和$\theta’$参数上的不同，而是说它们在表现上的不同的程度需要被限制，而想要这个区别被限制，就必须要使它可以被量化。因此，在PPO中，使用$KL(\theta,\theta’)$对$\theta$和$\theta’$在表现上的不同的程度进行量化。由此可以得到两个算法，即Proximal Policy Optimization(PPO)和TRPO(Trust Region Policy Optimization)：</p>
<ul>
<li>Proximal Policy Optimization(PPO)</li>
</ul>
<p>$$<br>J_{PPO}^{\theta’}(\theta) &#x3D; J^{\theta’}(\theta) - \beta KL(\theta,\theta’)\tag{17}<br>$$</p>
<ul>
<li>TRPO(Trust Region Policy Optimization)</li>
</ul>
<p>$$<br>J_{TRPO}^{\theta’}(\theta) &#x3D; E_{(s_t,a_t)\sim \pi_{\theta’}}[\frac{p_\theta(a_t|s_t)}{p_{\theta’}(a_t|s_t)}A^{\theta’}(s_t,a_t)]\tag{18}<br>$$</p>
<p>在TRPO中，$KL(\theta,\theta’)$以单独的限制存在，一般为$KL(\theta,\theta’)&lt;\delta$.</p>
<p>因此，PPO算法的伪代码如下：</p>
<ul>
<li><p>Initial policy parameters $\theta^0$</p>
</li>
<li><p>In each iteration:</p>
<ul>
<li>Using $\theta^k$ to interact with the environment to collect ${s_t,a_t}$ and compute advatage $A_{\theta^k}(st,at)$</li>
<li>Find $\theta$ optimizing $J_{PPO}(\theta)$</li>
<li>$J_{PPO}^{\theta^k} &#x3D; J^{\theta^k}(\theta) - \beta \times KL(\theta,\theta^k)$ # Update parameters several times</li>
</ul>
</li>
<li><p>与此同时，动态调整$\beta$:</p>
<ul>
<li>If $KL(\theta,\theta^k) &gt; KL_{max}$,increase $\beta$</li>
<li>If $KL(\theta,\theta^k) &lt; KL_{min}$,decrease $\beta$</li>
</ul>
</li>
</ul>
<p>这就完成了 KL Penalty的建立。</p>
<h2 id="第二种PPO算法"><a href="#第二种PPO算法" class="headerlink" title="第二种PPO算法"></a>第二种PPO算法</h2><p>在这个PPO算法中，并不是使用KL函数，而是使用clip函数，所谓的clip函数是指$clip(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)},1-\epsilon,1+\epsilon)$，当$\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)}$的值小于$1-\epsilon$时，它的值取$1-\epsilon$,当$\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)}$的值大于$1+\epsilon$时，它的值取$1+\epsilon$.因此可得：<br>$$<br>J_{PPO2}^{\theta^k}(\theta) \approx \sum_{(s_t,a_t)}min(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t|s_t)}A^{\theta^k}(s_t,a_t),clip(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)},1-\epsilon,1+\epsilon)A^{\theta^k}(s_t,a_t))<br>$$<br>$\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t|s_t)}$和$clip(\frac{p_\theta(a_t|s_t)}{p_\theta^k(a_t,s_t)},1-\epsilon,1+\epsilon)$的图像如图所示：</p>
<p><img src="http://cdn.leafii.top/img/secondppo.png" alt="secondppo" loading="lazy"></p>
<h2 id="PPO的网络结构"><a href="#PPO的网络结构" class="headerlink" title="PPO的网络结构"></a>PPO的网络结构</h2><p>未完待续。。。</p>
<h2 id="为什么说TRPO和PPO是On-policy的？"><a href="#为什么说TRPO和PPO是On-policy的？" class="headerlink" title="为什么说TRPO和PPO是On-policy的？"></a>为什么说TRPO和PPO是On-policy的？</h2><p><strong>首先我们明确什么是on-policy，什么是off-policy？</strong></p>
<ul>
<li>on-policy：就是要训练的agent跟环境互动的agent是同一个agent,也就是我们采样的网络和要优化的网络是否是同一个网络。</li>
<li>off-policy：那肯定就是跟上面相反的。</li>
</ul>
<p>那么进入正题，我们一般认为PPO是off-policy的原因就是因为PPO使用actor网络去sampler然后填充经验池，然后使用这个经验池中的数据去更新这个actor多个epoch，当更新到第二个epoch的时候那么actor就变成了actor1,然而经验池中的数据仍然是actor网络采样得到的，那么就造成了从更新第二个epoch开始采样的actor和要优化的actor不是同一个网络，那么可能就会认为它是off-pocliy的。</p>
<p>其实可以很简单的解释这个问题，根据off-policy的定义，采样的网络和要优化的网络不是一个网络，那么对于PPO来说，<strong>使用一批数据从更新actor的第二个epoch开始，数据虽然都是旧的actor采样得到的，但是我们并没有直接使用这批数据去更新我们的新的actor，而是使用imporance sampling先将数据分布不同导致的误差进行了修正</strong>。那么这个importance sampling的目的就是让这两者数据分布之间的差异尽可能的缩小，<strong>那么就可以近似理解成做了importance sampling之后的数据就是我们的更新（这里的更新指的是多个epoch更新的中间过程）后的actor采样得来的，这样就可以理解成我们要优化得actor和采样得actor是同一个actor，那么他就是on-policy的。</strong></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><p>【DRL-16】Proximal Policy Optimization : <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/142312072">https://zhuanlan.zhihu.com/p/142312072</a></p>
</li>
<li><p>为什么说TRPO和PPO是on-policy的？:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/387193698">https://zhuanlan.zhihu.com/p/387193698</a></p>
</li>
</ol>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Leafii</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="2022/09/20/强化学习_PPO算法(Proximal Policy Optimization)/" title="强化学习_PPO算法(Proximal Policy Optimization)">2022/09/20/强化学习_PPO算法(Proximal Policy Optimization)/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="../../../08/09/%E7%94%A8SHAP%E8%A7%A3%E9%87%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="next" title="用SHAP解释机器学习"><span class="post-nav-text">用SHAP解释机器学习</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2022 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> Leafii</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.9.3</span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="../../../../js/search/local-search.js" defer type="module"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="搜索..." value=""></div><div id="local-search-result"></div></div><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></div></body></html>